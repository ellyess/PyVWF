{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49835b69-bddd-4494-b8fa-50471618ed14",
   "metadata": {},
   "source": [
    "# Training method v2\n",
    "\n",
    "This training method will attempt to simulate the CF of all turbines in the designated spatial-temporal cluster which will be averaged to optimise the BC factors, this will better use individual power curves instead of assuming for a cluster.\n",
    "\n",
    "training data:\n",
    "turb info (lat, lon, capacity, height, model)\n",
    "cluster label for the turbine\n",
    "year and time res of simulated CF and obs CF\n",
    "the scalar calculated at the cluster+time res level\n",
    "\n",
    "so i need to produce an unsimulated CF, average that into the inputted time res per year and num clusters\n",
    "produce the training data described above\n",
    "farm offset can be run with dask like I did for v1 but  will select timeperiod for reanalaysis based on the time_res input\n",
    ", this would require me to change how speed is calcuulated and coverted to power and this should take inspo from my test functions as this is in a format to do multiple at a time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e2c5c2-7aec-4988-a453-393adf45f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask.dataframe as dd\n",
    "from scipy import interpolate\n",
    "from sklearn.cluster import KMeans\n",
    "import utm\n",
    "\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "from calendar import monthrange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from vwf.simulation import simulate_wind\n",
    "from vwf.extras import add_times\n",
    "# from vwf.preprocessing import (\n",
    "#     prep_era5,\n",
    "#     prep_obs,\n",
    "#     prep_obs_test,\n",
    "#     prep_merra2_method_1\n",
    "# )\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e9fb09-e124-4147-8601-a0d8c850cfe9",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64806abe-3abb-4af4-bd48-081d24fc63ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_obs(country, year_star, year_end):\n",
    "    \n",
    "    if country == \"DK\":\n",
    "        \"\"\"\n",
    "        For Denmark's data there had to be a lot of manual manipulation of the excel file. \n",
    "        I had to manually match the turbines that exist in the power curves file, with Denmarks naming convention then match it to the ID's. \n",
    "        anlaeg.xlsx is the raw file and match_turb_dk.xlsx is where the matching is done.\n",
    "        After this we are required to fill in missing turbine matches and also convert the coordinate system.\n",
    "        We also produce the observational data which is again manually seperated into yearly sheets from a megasheet for the years we desire.\n",
    "        As the observational data is power output we converted that to capacity factor with the matched turbines.\n",
    "        the ID's here are the gsrn ID\n",
    "        \"\"\"\n",
    "        ##############################\n",
    "        # producing turb_info\n",
    "        ##############################\n",
    "        # reading in the messy denmark turbine info that we have matched\n",
    "        df = pd.read_excel('data/wind_data/DK/raw/match_turb_dk.xlsx')\n",
    "        columns = ['Turbine identifier (GSRN)','Capacity (kW)','X (east) coordinate\\nUTM 32 Euref89','Y (north) coordinate\\nUTM 32 Euref89','Hub height (m)', 'Date of original connection to grid', 'turb_match']\n",
    "        df = df[columns]\n",
    "        rename_col = ['ID','capacity','x_east_32','y_north_32','height', 'date', 'model']\n",
    "        df.columns = rename_col\n",
    "        df = df.dropna()\n",
    "\n",
    "        # matching modelless turbines with closest model via capacity\n",
    "        metadata = pd.read_csv('data/turbine_info/models.csv')\n",
    "        metadata = metadata.sort_values('capacity')\n",
    "\n",
    "        df['model'][df['model'] == 0] = np.nan\n",
    "        df['capacity'] = df['capacity'].astype(int)\n",
    "        df = df.sort_values('capacity').reset_index(drop=True)\n",
    "        df.loc[df['model'].isna(), 'model'] = pd.merge_asof(df, metadata, left_on=[\"capacity\"], right_on=[\"capacity\"], direction=\"nearest\")['model_y']\n",
    "\n",
    "        # convert coordinate system\n",
    "        def rule(row):\n",
    "            lat, lon = utm.to_latlon(row[\"x_east_32\"], row[\"y_north_32\"], 32, 'W')\n",
    "            return pd.Series({\"lat\": lat, \"lon\": lon})\n",
    "\n",
    "        df = df.merge(df.apply(rule, axis=1), left_index= True, right_index= True)\n",
    "        df = df[['ID','capacity','lat','lon','height', 'date', 'model']]\n",
    "        df['ID'] = df['ID'].astype(str)\n",
    "        print(\"Number of turbines before preprocessing: \", len(df))\n",
    "        turb_info = df.drop(df[df['height'] < 1].index).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        ##############################\n",
    "        # producing obs_cf\n",
    "        ##############################\n",
    "\n",
    "        # Load observation data and slice the observed CF for chosen years\n",
    "        appended_data = []\n",
    "        for i in range(year_star, year_end+1): # change this back to +1 when i dont need 2020 obs\n",
    "            data = pd.read_excel('data/wind_data/DK/observation/Denmark_'+str(i)+'.xlsx')\n",
    "            data = data.iloc[3:,np.r_[0:1, 3:15]] # the slicing done here is file dependent please consider this when other files are used\n",
    "            data.columns = ['ID','1','2','3','4','5','6','7','8','9','10','11','12']\n",
    "            data['ID'] = data['ID'].astype(str)\n",
    "            data = data.reset_index(drop=True)\n",
    "            data['year'] = i\n",
    "\n",
    "            appended_data.append(data[:-1])\n",
    "\n",
    "        obs_gen = pd.concat(appended_data).reset_index(drop=True)\n",
    "        obs_gen.columns = [f'obs_{i}' if i not in ['ID', 'year'] else f'{i}' for i in obs_gen.columns]\n",
    "\n",
    "        # converting obs_gen into obs_cf by turning power into capacity factor\n",
    "        df = pd.merge(obs_gen, turb_info[['ID', 'capacity']],  how='left', on=['ID'])\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "        def daysDuringMonth(yy, m):\n",
    "            result = []    \n",
    "            [result.append(monthrange(y, m)[1]) for y in yy]        \n",
    "            return result\n",
    "\n",
    "        for i in range(1,13):\n",
    "            df['obs_'+str(i)] = df['obs_'+str(i)]/(((daysDuringMonth(df.year, i))*df['capacity'])*24)\n",
    "\n",
    "        df = df.drop(['capacity'], axis=1).reset_index(drop=True)\n",
    "        df['cf_max'] = df.iloc[:,1:13].max(axis=1)\n",
    "        df = df.drop(df[df['cf_max'] > 1].index)\n",
    "        df['cf_min'] = df.iloc[:,1:13].min(axis=1)\n",
    "        df = df.drop(df[df['cf_min'] <= 0.01].index)\n",
    "        df['cf_mean'] = df.iloc[:,1:13].mean(axis=1)\n",
    "        df = df.drop(df[df['cf_mean'] <= 0.01].index)\n",
    "        obs_cf = df.drop(['cf_mean', 'cf_max', 'cf_min'], axis=1).reset_index(drop=True)\n",
    "\n",
    "        obs_cf = obs_cf.loc[obs_cf['ID'].isin(turb_info['ID'])].reset_index(drop=True)\n",
    "        obs_cf = obs_cf[obs_cf.groupby('ID').ID.transform('count') == ((year_end-year_star)+1)].reset_index(drop=True)\n",
    "        obs_cf.columns = ['ID','1','2','3','4','5','6','7','8','9','10','11','12','year']\n",
    "        obs_cf = obs_cf.melt(id_vars=[\"ID\", \"year\"], \n",
    "                        var_name=\"month\", \n",
    "                        value_name=\"obs\")\n",
    "        # obs_cf.to_csv('data/wind_data/DK/obs_cf_train.csv', index = None)\n",
    "        \n",
    "        turb_info = turb_info.loc[turb_info['ID'].isin(obs_cf['ID'])].reset_index(drop=True)\n",
    "        # turb_info.to_csv('data/wind_data/DK/turb_info_train.csv', index = None)\n",
    "        \n",
    "        print(\"Number of turbines used in training: \", len(turb_info))\n",
    "        return obs_cf, turb_info\n",
    "\n",
    "# def prep_era5(year_star, year_end, train=False):\n",
    "def prep_era5(train=False):\n",
    "    \"\"\"\n",
    "    Reading a saved ERA5 file with 100m wind speeds and fsr.\n",
    "    changing names and converting wind speed components into wind speed.\n",
    "    \"\"\"\n",
    "    # Load the corresponding raw ERA5 file\n",
    "    if train == True:\n",
    "        ds = xr.open_mfdataset('data/reanalysis/train/*.nc')\n",
    "    else:\n",
    "        ds = xr.open_mfdataset('data/reanalysis/test/*.nc')\n",
    "    ds = ds.compute() # this allows it to not be dask chunks\n",
    "    \n",
    "    ds[\"wnd100m\"] = np.sqrt(ds[\"u100\"] ** 2 + ds[\"v100\"] ** 2).assign_attrs(\n",
    "        units=ds[\"u100\"].attrs[\"units\"], long_name=\"100 metre wind speed\"\n",
    "    )\n",
    "    \n",
    "    ds = ds.drop_vars([\"u100\", \"v100\"])\n",
    "    ds = ds.rename({\"fsr\": \"roughness\"})\n",
    "    \n",
    "    # turn hourly data into daily for speed of existing code\n",
    "    ds = ds.resample(time='1D').mean()\n",
    "    try:\n",
    "        ds = ds.rename({\"longitude\": \"lon\", \"latitude\": \"lat\"})\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    ds = ds.assign_coords(\n",
    "        lon=np.round(ds.lon.astype(float), 5), lat=np.round(ds.lat.astype(float), 5)\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def add_time_res(df):\n",
    "    df.loc[df['month'] == 1, ['bimonth','season']] = ['1/6', 'winter']\n",
    "    df.loc[df['month'] == 2, ['bimonth','season']] = ['1/6', 'winter']\n",
    "    df.loc[df['month'] == 3, ['bimonth','season']] = ['2/6', 'spring']\n",
    "    df.loc[df['month'] == 4, ['bimonth','season']] = ['2/6', 'spring']\n",
    "    df.loc[df['month'] == 5, ['bimonth','season']] = ['3/6', 'spring']\n",
    "    df.loc[df['month'] == 6, ['bimonth','season']] = ['3/6', 'summer']\n",
    "    df.loc[df['month'] == 7, ['bimonth','season']] = ['4/6', 'summer']\n",
    "    df.loc[df['month'] == 8, ['bimonth','season']] = ['4/6', 'summer']\n",
    "    df.loc[df['month'] == 9, ['bimonth','season']] = ['5/6', 'autumn']\n",
    "    df.loc[df['month'] == 10, ['bimonth','season']] = ['5/6', 'autumn']\n",
    "    df.loc[df['month'] == 11, ['bimonth','season']] = ['6/6', 'autumn']\n",
    "    df.loc[df['month'] == 12, ['bimonth','season']] = ['6/6', 'winter']\n",
    "    df['yearly'] = 'year'\n",
    "    return df\n",
    "\n",
    "    \n",
    "def match_generation_data(reanalysis, obs_cf, turb_info, powerCurveFile):\n",
    "    \n",
    "    sim_ws, sim_cf = simulate_wind(reanalysis, turb_info, powerCurveFile)\n",
    "    sim_cf = sim_cf.groupby(pd.Grouper(key='time',freq='M')).mean().reset_index()\n",
    "    sim_cf = sim_cf.melt(id_vars=[\"time\"], \n",
    "                    var_name=\"ID\", \n",
    "                    value_name=\"sim\")\n",
    "    sim_cf = add_times(sim_cf)\n",
    "    sim_cf = add_time_res(sim_cf)\n",
    "\n",
    "\n",
    "    sim_cf['ID'] = sim_cf['ID'].astype(str)\n",
    "    sim_cf['month'] = sim_cf['month'].astype(int)\n",
    "    sim_cf['year'] = sim_cf['year'].astype(int)\n",
    "    obs_cf['ID'] = obs_cf['ID'].astype(str)\n",
    "    obs_cf['month'] = obs_cf['month'].astype(int)\n",
    "    obs_cf['year'] = obs_cf['year'].astype(int)\n",
    "    \n",
    "    gen_cf = pd.merge(sim_cf, obs_cf, on=['ID', 'month', 'year'], how='left')\n",
    "    gen_cf = gen_cf.drop(['time'], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    return gen_cf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bca220c-fc9d-4333-8de3-8dad1c706737",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abf37714-7734-4109-96bf-7d65575a73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scalar(time_res, gen_data):\n",
    "    # scalar_alpha = 0.6\n",
    "    # scalar_beta = 0.2\n",
    "\n",
    "    # if time_res == 'year':\n",
    "    #     bias_data = gen_data.groupby(['year','cluster'], as_index=False)[['obs','sim']].mean()\n",
    "    #     bias_data['scalar'] = (scalar_alpha * (bias_data['obs'] / bias_data['sim'])) + scalar_beta\n",
    "    #     bias_data['time_slice'] = 'year'\n",
    "        \n",
    "    # else:\n",
    "    bias_data = gen_data.groupby([time_res, 'cluster', 'year'])[['obs','sim']].mean()\n",
    "    # bias_data['scalar'] = (scalar_alpha * (bias_data['obs'] / bias_data['sim'])) + scalar_beta\n",
    "    bias_data['scalar'] = bias_data['obs'] / bias_data['sim']\n",
    "    bias_data = bias_data.reset_index()\n",
    "    bias_data.columns = ['time_slice', 'cluster', 'year', 'obs', 'sim', 'scalar']\n",
    "        \n",
    "    return bias_data[['year', 'time_slice', 'cluster', 'obs', 'sim', 'scalar']]\n",
    "\n",
    "\n",
    "def cluster_turbines(num_clu, turb_info):\n",
    "    # generating the cluster labels \n",
    "    kmeans = KMeans(\n",
    "        init=\"random\",\n",
    "        n_clusters = num_clu,\n",
    "        n_init = 10,\n",
    "        max_iter = 300,\n",
    "        random_state = 42\n",
    "    )\n",
    "    \n",
    "    lat = turb_info['lat']\n",
    "    lon = turb_info['lon']\n",
    "    df = pd.DataFrame(list(zip(lat, lon)), columns =['lat', 'lon'])\n",
    "    kmeans.fit(df)\n",
    "    turb_info['cluster'] = kmeans.labels_\n",
    "    turb_info.to_csv('data/results/new_factors/clusters/clus_info_'+str(num_clu)+'.csv')\n",
    "    return turb_info\n",
    "    \n",
    "def train_data(time_res, gen_cf, clus_info):\n",
    "\n",
    "    if time_res == 'yearly':\n",
    "        gen_data = gen_cf.groupby(['year','ID'], as_index=False)[['obs','sim']].mean()\n",
    "        gen_data['yearly'] = 'year'\n",
    "        \n",
    "    else:\n",
    "        gen_data = gen_cf.groupby(['year',time_res,'ID'], as_index=False)[['obs','sim']].mean()\n",
    "\n",
    "    gen_data = pd.merge(gen_data, clus_info[['ID', 'cluster', 'lon', 'lat', 'capacity', 'height', 'model']], on='ID', how='left')\n",
    "    bias_data = calculate_scalar(time_res, gen_data)\n",
    "\n",
    "    return bias_data\n",
    "\n",
    "def simulate_wind_speed(reanalysis, turb_info):\n",
    "    reanalysis = reanalysis.assign_coords(\n",
    "        height=('height', turb_info['height'].unique()))\n",
    "    \n",
    "    # calculating wind speed from reanalysis dataset variables\n",
    "    ws = reanalysis.wnd100m * (np.log(reanalysis.height/ reanalysis.roughness) / np.log(100 / reanalysis.roughness))\n",
    "    \n",
    "    # creating coordinates to spatially interpolate to\n",
    "    lat =  xr.DataArray(turb_info['lat'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "    lon =  xr.DataArray(turb_info['lon'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "    height =  xr.DataArray(turb_info['height'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "\n",
    "    # spatial interpolating to turbine positions\n",
    "    sim_ws = ws.interp(\n",
    "            lon=lon, lat=lat, height=height,\n",
    "            kwargs={\"fill_value\": None})\n",
    "    \n",
    "    return sim_ws\n",
    "\n",
    "def speed_to_power(column, powerCurveFile, turb_info):\n",
    "    x = powerCurveFile['data$speed']\n",
    "    turb_name = turb_info.loc[turb_info['ID'] == column.name, 'model']           \n",
    "    y = powerCurveFile[turb_name].to_numpy().flatten()\n",
    "    f = interpolate.Akima1DInterpolator(x, y)\n",
    "    return f(column)\n",
    "\n",
    "\n",
    "def simulate_wind_train(reanalysis, turb_info, powerCurveFile, scalar=1, offset=0): \n",
    "    # calculating wind speed from reanalysis data\n",
    "    sim_ws = simulate_wind_speed(reanalysis, turb_info)\n",
    "    unc_ws = sim_ws.to_pandas()\n",
    "    cor_ws = (unc_ws * scalar) + offset\n",
    "    # converting to power\n",
    "    cor_cf = cor_ws.apply(speed_to_power, args=(powerCurveFile, turb_info), axis=0)\n",
    "    return np.mean(cor_cf)\n",
    "\n",
    "\n",
    "def find_offset(row, turb_info, reanalysis, powerCurveFile):\n",
    "\n",
    "    # start_time = time.time()\n",
    "    if row['time_slice'] == 'spring':\n",
    "        time_slice = [3,4,5]\n",
    "    elif row['time_slice'] == 'summer':\n",
    "        time_slice = [6,7,8]\n",
    "    elif row['time_slice'] == 'autumn':\n",
    "        time_slice = [9,10,11]\n",
    "    elif row['time_slice'] == 'winter':\n",
    "        time_slice = [1,2,12]\n",
    "    elif row['time_slice'] == '1/6':\n",
    "        time_slice = [1,2]\n",
    "    elif row['time_slice'] == '2/6':\n",
    "        time_slice = [3,4]\n",
    "    elif row['time_slice'] == '3/6':\n",
    "        time_slice = [5,6]\n",
    "    elif row['time_slice'] == '4/6':\n",
    "        time_slice = [7,8]\n",
    "    elif row['time_slice'] == '5/6':\n",
    "        time_slice = [9,10]\n",
    "    elif row['time_slice'] == '6/6':\n",
    "        time_slice = [11,12]\n",
    "    elif row['time_slice'] == 'year':\n",
    "        time_slice = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "    else:\n",
    "        time_slice = int(row['time_slice'])\n",
    "    \n",
    "    # end_time = time.time()\n",
    "    # elapsed_time = end_time - start_time\n",
    "    # print(\"If statements took: \", elapsed_time)\n",
    "    \n",
    "    # decide our initial search step size\n",
    "    stepSize = -0.64\n",
    "    if (row.sim > row.obs):\n",
    "        stepSize = 0.64\n",
    "        \n",
    "    # start_time = time.time()\n",
    "    myOffset = 0\n",
    "    while np.abs(stepSize) > 0.002: # Stop when step-size is smaller than our power curve's resolution\n",
    "        myOffset += stepSize # If we are still far from energytarget, increase stepsize\n",
    "        \n",
    "        # calculate the mean simulated CF using the new offset\n",
    "        mean_sim_cf = simulate_wind_train(\n",
    "            reanalysis.sel(\n",
    "                    time=np.logical_and(\n",
    "                    reanalysis.time.dt.year == row.year, \n",
    "                    reanalysis.time.dt.month.isin(time_slice)\n",
    "                )\n",
    "            ),\n",
    "            turb_info.loc[turb_info['cluster'] == row.cluster],\n",
    "            powerCurveFile, \n",
    "            row.scalar, \n",
    "            myOffset\n",
    "        )\n",
    "        \n",
    "        # if we have overshot our target, then repeat, searching the other direction\n",
    "        # ((guess < target & sign(step) < 0) | (guess > target & sign(step) > 0))\n",
    "        if mean_sim_cf != 0:\n",
    "            if np.sign(mean_sim_cf - row.obs) == np.sign(stepSize):\n",
    "                stepSize = -stepSize / 2\n",
    "            # If we have reached unreasonable places, stop\n",
    "            if myOffset < -20 or myOffset > 20:\n",
    "                break\n",
    "        elif mean_cf == 0:\n",
    "            myOffset = 0\n",
    "            break\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed_time = end_time - start_time\n",
    "    # print(\"While loop took: \", elapsed_time)\n",
    "    return myOffset\n",
    "\n",
    "\n",
    "######################## Test\n",
    "def closest_cluster(clus_info, turb_info):\n",
    "    \"\"\"\n",
    "    Assign turbines not found in training data to closest cluster.\n",
    "    \"\"\"\n",
    "    # making sure ID column dtype is same   \n",
    "    clus_info['ID'] = clus_info['ID'].astype(str)\n",
    "    turb_info['ID'] = turb_info['ID'].astype(str)\n",
    "    \n",
    "    avg = clus_info.groupby(['cluster'], as_index=False)[['lat','lon']].mean()\n",
    "    turb_info = pd.DataFrame.merge(clus_info[['ID','cluster']], turb_info, on='ID', how='right')\n",
    "\n",
    "    for i in range(len(turb_info)):\n",
    "        if np.isnan(turb_info.cluster[i]) == True:\n",
    "            # Find the cluster center closest to the new turbine\n",
    "            # - find smallest distance between the new turbine and cluster centers\n",
    "            indx = np.argmin(np.sqrt((avg.lat.values - turb_info.lat[i])**2 + (avg.lon.values - turb_info.lon[i])**2))\n",
    "            turb_info.cluster[i] = avg.cluster[indx]\n",
    "\n",
    "    turb_info = turb_info.reset_index(drop=True)\n",
    "\n",
    "    return turb_info\n",
    "\n",
    "\n",
    "def simulate_wind(reanalysis, turb_info, powerCurveFile, *args): \n",
    "    # calculating wind speed from reanalysis data\n",
    "    # start_time = time.time()\n",
    "    sim_ws = simulate_wind_speed(reanalysis, turb_info)\n",
    "    sim_ws = sim_ws.to_pandas()\n",
    "\n",
    "    if len(args) >= 1: \n",
    "        bc_factors = args[0]\n",
    "        \n",
    "        sim_ws = sim_ws.reset_index()\n",
    "        sim_ws = sim_ws.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "            var_name=\"ID\", \n",
    "            value_name=\"ws\")\n",
    "\n",
    "\n",
    "        sim_ws = add_times(sim_ws)\n",
    "        sim_ws = add_time_res(sim_ws)\n",
    "        \n",
    "        sim_ws['month'] = sim_ws['month'].astype(str)\n",
    "        bc_factors[time_res] = bc_factors[time_res].astype(str)\n",
    "        # turb_info['ID'] = turb_info['ID'].astype(str)\n",
    "        # bc_factors['ID'] = bc_factors['ID'].astype(str)\n",
    "        sim_ws = pd.merge(sim_ws, turb_info[['ID','cluster']], on=['ID'], how='left')\n",
    "        sim_ws = pd.merge(sim_ws, bc_factors, on=['cluster',time_res], how='left')\n",
    "        sim_ws['ws'] = (sim_ws.ws * sim_ws.scalar) + sim_ws.offset # equation 2\n",
    "        sim_ws = sim_ws.pivot(index=['time'], columns='ID', values='ws')\n",
    "\n",
    "    sim_cf = sim_ws.apply(speed_to_power, args=(powerCurveFile, turb_info), axis=0)\n",
    "\n",
    "    return sim_ws.reset_index(), sim_cf.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a08f7c-f418-42f9-9d3e-e8ad1bff3d5a",
   "metadata": {},
   "source": [
    "## Full train\n",
    "currently trying to speed up the speed to power so I can run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43fb715-c591-4082-8108-bf73f3f35840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of turbines before preprocessing:  5682\n",
      "Number of turbines used in training:  3712\n"
     ]
    }
   ],
   "source": [
    "year_star = 2015 # start year of training period\n",
    "year_end = 2019 # end year of training period\n",
    "year_test = 2020 # year you wish to receive a time series for\n",
    "\n",
    "powerCurveFileLoc = 'data/turbine_info/Wind Turbine Power Curves.csv'\n",
    "powerCurveFile = pd.read_csv(powerCurveFileLoc)\n",
    "\n",
    "train_era5 = prep_era5(True)\n",
    "train_obs_cf, train_turb_info = prep_obs(\"DK\", year_star, year_end)\n",
    "train_gen_cf = match_generation_data(train_era5, train_obs_cf, train_turb_info, powerCurveFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a465f5-5613-4f47-ba65-93b2cf1e7764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for  1  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 167.87 seconds\n",
      " \n",
      "Train for  1  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 181.74 seconds\n",
      " \n",
      "Train for  1  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 186.63 seconds\n",
      " \n",
      "Train for  1  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 367.23 seconds\n",
      " \n",
      "Train for  2  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 114.87 seconds\n",
      " \n",
      "Train for  2  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 108.25 seconds\n",
      " \n",
      "Train for  2  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 173.53 seconds\n",
      " \n",
      "Train for  2  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 280.84 seconds\n",
      " \n",
      "Train for  3  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 81.88 seconds\n",
      " \n",
      "Train for  3  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 114.05 seconds\n",
      " \n",
      "Train for  3  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 170.59 seconds\n",
      " \n",
      "Train for  3  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 267.08 seconds\n",
      " \n",
      "Train for  5  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 47.12 seconds\n",
      " \n",
      "Train for  5  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 107.12 seconds\n",
      " \n",
      "Train for  5  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 139.13 seconds\n",
      " \n",
      "Train for  5  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 303.43 seconds\n",
      " \n",
      "Train for  10  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 60.95 seconds\n",
      " \n",
      "Train for  10  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 122.48 seconds\n",
      " \n",
      "Train for  10  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 143.24 seconds\n",
      " \n",
      "Train for  10  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 262.50 seconds\n",
      " \n",
      "Train for  15  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 46.45 seconds\n",
      " \n",
      "Train for  15  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 112.52 seconds\n",
      " \n",
      "Train for  15  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 148.17 seconds\n",
      " \n",
      "Train for  15  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 263.03 seconds\n",
      " \n",
      "Train for  20  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 58.12 seconds\n",
      " \n",
      "Train for  20  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 119.07 seconds\n",
      " \n",
      "Train for  20  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 152.97 seconds\n",
      " \n",
      "Train for  20  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 282.07 seconds\n",
      " \n",
      "Train for  30  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 53.33 seconds\n",
      " \n",
      "Train for  30  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 114.32 seconds\n",
      " \n",
      "Train for  30  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 176.09 seconds\n",
      " \n",
      "Train for  30  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 326.87 seconds\n",
      " \n",
      "Train for  50  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 60.08 seconds\n",
      " \n",
      "Train for  50  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 124.73 seconds\n",
      " \n",
      "Train for  50  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 164.63 seconds\n",
      " \n",
      "Train for  50  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 304.41 seconds\n",
      " \n",
      "Train for  100  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 68.03 seconds\n",
      " \n",
      "Train for  100  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 151.75 seconds\n",
      " \n",
      "Train for  100  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 195.11 seconds\n",
      " \n",
      "Train for  100  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 341.47 seconds\n",
      " \n",
      "Train for  150  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 77.00 seconds\n",
      " \n",
      "Train for  150  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 157.32 seconds\n",
      " \n",
      "Train for  150  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 221.59 seconds\n",
      " \n",
      "Train for  150  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 390.15 seconds\n",
      " \n",
      "Train for  200  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 80.12 seconds\n",
      " \n",
      "Train for  200  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 181.25 seconds\n",
      " \n",
      "Train for  200  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 241.89 seconds\n",
      " \n",
      "Train for  200  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 470.18 seconds\n",
      " \n",
      "Train for  300  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 98.73 seconds\n",
      " \n",
      "Train for  300  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 227.81 seconds\n",
      " \n",
      "Train for  300  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 322.61 seconds\n",
      " \n",
      "Train for  300  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 566.64 seconds\n",
      " \n",
      "Train for  400  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 111.39 seconds\n",
      " \n",
      "Train for  400  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 252.32 seconds\n",
      " \n",
      "Train for  400  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 349.20 seconds\n",
      " \n",
      "Train for  400  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 638.19 seconds\n",
      " \n"
     ]
    }
   ],
   "source": [
    "cluster_list = [1,2,3,5,10,15,20,30,50,100,150,200,300,400]\n",
    "time_res_list = ['yearly', 'season', 'bimonth', 'month'] \n",
    "\n",
    "# cluster_list = [1,2]\n",
    "# time_res_list = ['yearly', 'season', 'bimonth', 'month']\n",
    "\n",
    "for num_clu in cluster_list:\n",
    "    train_clus_info = cluster_turbines(num_clu, train_turb_info)\n",
    "\n",
    "    for time_res in time_res_list:\n",
    "        print(\"Train for \", num_clu, \" clusters with time resolution: \", time_res, \" is taking place.\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        bias_data = train_data(time_res, train_gen_cf, train_clus_info)\n",
    "        ddf = dd.from_pandas(bias_data, npartitions=40)\n",
    "        \n",
    "        def find_offset_parallel(df):\n",
    "            return df.apply(find_offset, args=(train_clus_info, train_era5, powerCurveFile), axis=1)\n",
    "            \n",
    "        ddf[\"offset\"] = ddf.map_partitions(find_offset_parallel, meta=('offset', 'float'))\n",
    "        ddf.to_csv('data/results/new_factors/'+time_res+'_'+str(num_clu)+'.csv', single_file=True, compute_kwargs={'scheduler':'processes'})\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(\"Results completed and saved. Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "        print(\" \")\n",
    "\n",
    "## this works\n",
    "# for num_clu, time_res in itertools.product(cluster_list, time_res_list):\n",
    "#     print(\"Train for \", num_clu, \" clusters with time resolution: \", time_res, \" is taking place.\")\n",
    "#     start_time = time.time()\n",
    "#     gen_data, bias_data = train_data(num_clu, time_res, train_gen_cf, train_turb_info)\n",
    "#     # ddf = dd.from_pandas(bias_data, npartitions=40)\n",
    "#     # ddf[\"offset\"] = ddf.map_partitions(find_offset_parallel, meta=('offset', 'float'))\n",
    "#     # ddf.to_csv('data/results/new_factors/'+time_res+'_'+str(num_clu)+'.csv', single_file=True, compute_kwargs={'scheduler':'processes'})\n",
    "#     end_time = time.time()\n",
    "#     elapsed_time = end_time - start_time\n",
    "#     print(\"Results completed and saved. Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "#     print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961edd1a-688d-402b-b9ba-ab7a5c2de01f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64127889-2ab1-41f7-8ec1-3b24dc32e271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vwf.preprocessing import prep_obs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f51aa5d-40c0-4cf4-b567-232751e228f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for  1  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.35 seconds\n",
      " \n",
      "Test for  1  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.95 seconds\n",
      " \n",
      "Test for  1  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.88 seconds\n",
      " \n",
      "Test for  1  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.91 seconds\n",
      " \n",
      "Test for  2  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.80 seconds\n",
      " \n",
      "Test for  2  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.82 seconds\n",
      " \n",
      "Test for  2  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.76 seconds\n",
      " \n",
      "Test for  2  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.81 seconds\n",
      " \n",
      "Test for  3  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.02 seconds\n",
      " \n",
      "Test for  3  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.02 seconds\n",
      " \n",
      "Test for  3  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.05 seconds\n",
      " \n",
      "Test for  3  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.88 seconds\n",
      " \n",
      "Test for  5  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.80 seconds\n",
      " \n",
      "Test for  5  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.86 seconds\n",
      " \n",
      "Test for  5  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.00 seconds\n",
      " \n",
      "Test for  5  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.99 seconds\n",
      " \n",
      "Test for  10  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.00 seconds\n",
      " \n",
      "Test for  10  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.83 seconds\n",
      " \n",
      "Test for  10  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.89 seconds\n",
      " \n",
      "Test for  10  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.95 seconds\n",
      " \n",
      "Test for  15  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.94 seconds\n",
      " \n",
      "Test for  15  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.92 seconds\n",
      " \n",
      "Test for  15  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.88 seconds\n",
      " \n",
      "Test for  15  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.92 seconds\n",
      " \n",
      "Test for  20  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.93 seconds\n",
      " \n",
      "Test for  20  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.90 seconds\n",
      " \n",
      "Test for  20  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.89 seconds\n",
      " \n",
      "Test for  20  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.93 seconds\n",
      " \n",
      "Test for  30  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.91 seconds\n",
      " \n",
      "Test for  30  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.87 seconds\n",
      " \n",
      "Test for  30  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.80 seconds\n",
      " \n",
      "Test for  30  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.92 seconds\n",
      " \n",
      "Test for  50  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.11 seconds\n",
      " \n",
      "Test for  50  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.11 seconds\n",
      " \n",
      "Test for  50  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.08 seconds\n",
      " \n",
      "Test for  50  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.12 seconds\n",
      " \n",
      "Test for  100  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.05 seconds\n",
      " \n",
      "Test for  100  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.95 seconds\n",
      " \n",
      "Test for  100  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.89 seconds\n",
      " \n",
      "Test for  100  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.20 seconds\n",
      " \n",
      "Test for  150  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.18 seconds\n",
      " \n",
      "Test for  150  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.33 seconds\n",
      " \n",
      "Test for  150  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.10 seconds\n",
      " \n",
      "Test for  150  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.19 seconds\n",
      " \n",
      "Test for  200  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.11 seconds\n",
      " \n",
      "Test for  200  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.02 seconds\n",
      " \n",
      "Test for  200  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.90 seconds\n",
      " \n",
      "Test for  200  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.01 seconds\n",
      " \n",
      "Test for  300  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.99 seconds\n",
      " \n",
      "Test for  300  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.86 seconds\n",
      " \n",
      "Test for  300  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.80 seconds\n",
      " \n",
      "Test for  300  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 8.00 seconds\n",
      " \n",
      "Test for  400  clusters with time resolution:  yearly  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.97 seconds\n",
      " \n",
      "Test for  400  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.98 seconds\n",
      " \n",
      "Test for  400  clusters with time resolution:  bimonth  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.86 seconds\n",
      " \n",
      "Test for  400  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 7.94 seconds\n",
      " \n"
     ]
    }
   ],
   "source": [
    "year_test = 2020\n",
    "test_era5 = prep_era5()\n",
    "test_turb_info = prep_obs_test(\"DK\", year_test)\n",
    "\n",
    "cluster_list = [1,2,3,5,10,15,20,30,50,100,150,200,300,400]\n",
    "time_res_list = ['yearly', 'season', 'bimonth', 'month'] \n",
    "# cluster_list = [1,2]\n",
    "# time_res_list = ['month']\n",
    "\n",
    "\n",
    "# running the full test results\n",
    "unc_ws, unc_cf = simulate_wind(test_era5, test_turb_info, powerCurveFile)\n",
    "unc_ws.to_csv('data/results/raw/'+str(year_test)+'_unc_ws.csv', index = None)\n",
    "unc_cf.to_csv('data/results/raw/'+str(year_test)+'_unc_cf.csv', index = None)\n",
    "\n",
    "for num_clu in cluster_list:\n",
    "    train_clus_info = pd.read_csv('data/results/new_factors/clusters/clus_info_'+str(num_clu)+'.csv')\n",
    "    test_clus_info = closest_cluster(train_clus_info, test_turb_info)\n",
    "\n",
    "    for time_res in time_res_list:\n",
    "        print(\"Test for \", num_clu, \" clusters with time resolution: \", time_res, \" is taking place.\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        bias_data = pd.read_csv('data/results/new_factors/'+time_res+'_'+str(num_clu)+'.csv')\n",
    "        bc_factors = bias_data.groupby(['cluster', 'time_slice'], as_index=False).agg({'scalar': 'mean', 'offset': 'mean'})\n",
    "        bc_factors.columns = ['cluster',time_res,'scalar','offset']\n",
    "\n",
    "        cor_ws, cor_cf = simulate_wind(test_era5, test_clus_info, powerCurveFile, bc_factors)\n",
    "        cor_ws.to_csv('data/results/raw/'+str(year_test)+'_'+time_res+'_'+str(num_clu)+'_cor_ws.csv', index = None)\n",
    "        cor_cf.to_csv('data/results/raw/'+str(year_test)+'_'+time_res+'_'+str(num_clu)+'_cor_cf.csv', index = None)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(\"Results completed and saved. Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9defa15b-de04-46ab-a564-f72c62419769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>yearly</th>\n",
       "      <th>scalar</th>\n",
       "      <th>offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>year</td>\n",
       "      <td>0.801406</td>\n",
       "      <td>0.8125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster yearly    scalar  offset\n",
       "0        0   year  0.801406  0.8125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ef79c-9c4c-443e-ab9c-6102dbc9efc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f3ec0-f8b7-4110-b27f-1edf561fca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_ws = unc_ws.reset_index()\n",
    "sim_ws = sim_ws.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "    var_name=\"ID\", \n",
    "    value_name=\"ws\")\n",
    "sim_ws = add_times(sim_ws)\n",
    "sim_ws = add_time_res(sim_ws)\n",
    "sim_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9d0e74-0f37-459c-8a49-4c808d36a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f196ece-e7b3-4d54-a85a-60036068d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eef7ba-e15e-4142-9985-c343e5901f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = cor_cf.reset_index()\n",
    "cor = cor.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "        var_name=\"ID\", \n",
    "        value_name=\"cor\")\n",
    "cor_month = cor.groupby([pd.Grouper(key='time',freq='M')])['cor'].mean().reset_index()\n",
    "    \n",
    "unc = unc_cf.reset_index()\n",
    "unc = unc.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "    var_name=\"ID\", \n",
    "    value_name=\"unc\")\n",
    "unc_month = unc.groupby([pd.Grouper(key='time',freq='M')])['unc'].mean().reset_index()\n",
    "\n",
    "obs = pd.read_csv('data/wind_data/DK/obs_cf_test.csv', parse_dates=['time'])\n",
    "obs = obs.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "    var_name=\"ID\", \n",
    "    value_name=\"obs\")\n",
    "obs_month = obs.groupby([pd.Grouper(key='time',freq='M')])['obs'].mean().reset_index()\n",
    "\n",
    "cf_month = obs_month.merge(unc_month,on=['time']).merge(cor_month,on=['time'])\n",
    "cf_month.columns = [\"time\", 'obs', 'unc','cor']\n",
    "cf_month[\"time\"] = cf_month[\"time\"].dt.month\n",
    "cf_month = cf_month.melt(id_vars=[\"time\"],\n",
    "            var_name=\"model\", \n",
    "            value_name=\"CF\")\n",
    "\n",
    "\n",
    "sns.lineplot(\n",
    "    data = cf_month,\n",
    "    x=\"time\",\n",
    "    y=\"CF\",\n",
    "    hue=\"model\",\n",
    "    style=\"model\",\n",
    "    legend = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559aea12-a061-4ba2-919e-2247fc9a123a",
   "metadata": {},
   "source": [
    "### Attempt at changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24d4c773-37ee-427b-baca-f45d72d5f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utm\n",
    "\n",
    "def prep_obs(country, year_star, year_end):\n",
    "    \n",
    "    if country == \"DK\":\n",
    "        \"\"\"\n",
    "        For Denmark's data there had to be a lot of manual manipulation of the excel file. \n",
    "        I had to manually match the turbines that exist in the power curves file, with Denmarks naming convention then match it to the ID's. \n",
    "        anlaeg.xlsx is the raw file and match_turb_dk.xlsx is where the matching is done.\n",
    "        After this we are required to fill in missing turbine matches and also convert the coordinate system.\n",
    "        We also produce the observational data which is again manually seperated into yearly sheets from a megasheet for the years we desire.\n",
    "        As the observational data is power output we converted that to capacity factor with the matched turbines.\n",
    "        the ID's here are the gsrn ID\n",
    "        \"\"\"\n",
    "        ##############################\n",
    "        # producing turb_info\n",
    "        ##############################\n",
    "        # reading in the messy denmark turbine info that we have matched\n",
    "        df = pd.read_excel('data/wind_data/DK/raw/match_turb_dk.xlsx')\n",
    "        columns = ['Turbine identifier (GSRN)','Capacity (kW)','X (east) coordinate\\nUTM 32 Euref89','Y (north) coordinate\\nUTM 32 Euref89','Hub height (m)', 'Date of original connection to grid', 'turb_match']\n",
    "        df = df[columns]\n",
    "        rename_col = ['ID','capacity','x_east_32','y_north_32','height', 'date', 'model']\n",
    "        df.columns = rename_col\n",
    "        df = df.dropna()\n",
    "\n",
    "        # matching modelless turbines with closest model via capacity\n",
    "        metadata = pd.read_csv('data/turbine_info/models.csv')\n",
    "        metadata = metadata.sort_values('capacity')\n",
    "\n",
    "        df['model'][df['model'] == 0] = np.nan\n",
    "        df['capacity'] = df['capacity'].astype(int)\n",
    "        df = df.sort_values('capacity').reset_index(drop=True)\n",
    "        df.loc[df['model'].isna(), 'model'] = pd.merge_asof(df, metadata, left_on=[\"capacity\"], right_on=[\"capacity\"], direction=\"nearest\")['model_y']\n",
    "\n",
    "        # convert coordinate system\n",
    "        def rule(row):\n",
    "            lat, lon = utm.to_latlon(row[\"x_east_32\"], row[\"y_north_32\"], 32, 'W')\n",
    "            return pd.Series({\"lat\": lat, \"lon\": lon})\n",
    "\n",
    "        df = df.merge(df.apply(rule, axis=1), left_index= True, right_index= True)\n",
    "        df = df[['ID','capacity','lat','lon','height', 'date', 'model']]\n",
    "        df['ID'] = df['ID'].astype(str)\n",
    "        print(\"Number of turbines before preprocessing: \", len(df))\n",
    "        turb_info = df.drop(df[df['height'] < 1].index).reset_index(drop=True)\n",
    "\n",
    "\n",
    "        ##############################\n",
    "        # producing obs_cf\n",
    "        ##############################\n",
    "\n",
    "        # Load observation data and slice the observed CF for chosen years\n",
    "        appended_data = []\n",
    "        for i in range(year_star, year_end+1): # change this back to +1 when i dont need 2020 obs\n",
    "            data = pd.read_excel('data/wind_data/DK/observation/Denmark_'+str(i)+'.xlsx')\n",
    "            data = data.iloc[3:,np.r_[0:1, 3:15]] # the slicing done here is file dependent please consider this when other files are used\n",
    "            data.columns = ['ID','1','2','3','4','5','6','7','8','9','10','11','12']\n",
    "            data['ID'] = data['ID'].astype(str)\n",
    "            data = data.reset_index(drop=True)\n",
    "            data['year'] = i\n",
    "\n",
    "            appended_data.append(data[:-1])\n",
    "\n",
    "        obs_gen = pd.concat(appended_data).reset_index(drop=True)\n",
    "        obs_gen.columns = [f'obs_{i}' if i not in ['ID', 'year'] else f'{i}' for i in obs_gen.columns]\n",
    "\n",
    "        # converting obs_gen into obs_cf by turning power into capacity factor\n",
    "        df = pd.merge(obs_gen, turb_info[['ID', 'capacity']],  how='left', on=['ID'])\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "        def daysDuringMonth(yy, m):\n",
    "            result = []    \n",
    "            [result.append(monthrange(y, m)[1]) for y in yy]        \n",
    "            return result\n",
    "\n",
    "        for i in range(1,13):\n",
    "            df['obs_'+str(i)] = df['obs_'+str(i)]/(((daysDuringMonth(df.year, i))*df['capacity'])*24)\n",
    "\n",
    "        df = df.drop(['capacity'], axis=1).reset_index(drop=True)\n",
    "        df['cf_max'] = df.iloc[:,1:13].max(axis=1)\n",
    "        df = df.drop(df[df['cf_max'] > 1].index)\n",
    "        df['cf_min'] = df.iloc[:,1:13].min(axis=1)\n",
    "        df = df.drop(df[df['cf_min'] <= 0.01].index)\n",
    "        df['cf_mean'] = df.iloc[:,1:13].mean(axis=1)\n",
    "        df = df.drop(df[df['cf_mean'] <= 0.01].index)\n",
    "        obs_cf = df.drop(['cf_mean', 'cf_max', 'cf_min'], axis=1).reset_index(drop=True)\n",
    "\n",
    "        obs_cf = obs_cf.loc[obs_cf['ID'].isin(turb_info['ID'])].reset_index(drop=True)\n",
    "        obs_cf = obs_cf[obs_cf.groupby('ID').ID.transform('count') == ((year_end-year_star)+1)].reset_index(drop=True)\n",
    "        obs_cf.columns = ['ID','1','2','3','4','5','6','7','8','9','10','11','12','year']\n",
    "        obs_cf = obs_cf.melt(id_vars=[\"ID\", \"year\"], \n",
    "                        var_name=\"month\", \n",
    "                        value_name=\"obs\")\n",
    "        # obs_cf.to_csv('data/wind_data/DK/obs_cf_train.csv', index = None)\n",
    "        \n",
    "        turb_info = turb_info.loc[turb_info['ID'].isin(obs_cf['ID'])].reset_index(drop=True)\n",
    "        # turb_info.to_csv('data/wind_data/DK/turb_info_train.csv', index = None)\n",
    "        \n",
    "        print(\"Number of turbines used in training: \", len(turb_info))\n",
    "        return obs_cf, turb_info\n",
    "\n",
    "\n",
    "def add_time_res(df):\n",
    "    df.loc[df['month'] == 1, ['bimonth','season']] = ['1/6', 'winter']\n",
    "    df.loc[df['month'] == 2, ['bimonth','season']] = ['1/6', 'winter']\n",
    "    df.loc[df['month'] == 3, ['bimonth','season']] = ['2/6', 'spring']\n",
    "    df.loc[df['month'] == 4, ['bimonth','season']] = ['2/6', 'spring']\n",
    "    df.loc[df['month'] == 5, ['bimonth','season']] = ['3/6', 'spring']\n",
    "    df.loc[df['month'] == 6, ['bimonth','season']] = ['3/6', 'summer']\n",
    "    df.loc[df['month'] == 7, ['bimonth','season']] = ['4/6', 'summer']\n",
    "    df.loc[df['month'] == 8, ['bimonth','season']] = ['4/6', 'summer']\n",
    "    df.loc[df['month'] == 9, ['bimonth','season']] = ['5/6', 'autumn']\n",
    "    df.loc[df['month'] == 10, ['bimonth','season']] = ['5/6', 'autumn']\n",
    "    df.loc[df['month'] == 11, ['bimonth','season']] = ['6/6', 'autumn']\n",
    "    df.loc[df['month'] == 12, ['bimonth','season']] = ['6/6', 'winter']\n",
    "    df['year_res'] = 'year'\n",
    "    return df\n",
    "\n",
    "def match_generation_data(reanalysis, obs_cf, turb_info, powerCurveFile):\n",
    "\n",
    "    sim_ws, sim_cf = simulate_wind_test(reanalysis, turb_info, powerCurveFile)\n",
    "    \n",
    "    sim_ws = sim_ws.melt(id_vars=[\"time\"], \n",
    "                    var_name=\"ID\", \n",
    "                    value_name=\"ws\")\n",
    "    sim_ws = add_times(sim_ws)\n",
    "    sim_ws = add_time_res(sim_ws)\n",
    "\n",
    "    \n",
    "    sim_cf = sim_cf.groupby(pd.Grouper(key='time',freq='M')).mean().reset_index()\n",
    "    sim_cf = sim_cf.melt(id_vars=[\"time\"], \n",
    "                    var_name=\"ID\", \n",
    "                    value_name=\"sim\")\n",
    "    sim_cf = add_times(sim_cf)\n",
    "    sim_cf = add_time_res(sim_cf)\n",
    "\n",
    "    sim_cf['ID'] = sim_cf['ID'].astype(str)\n",
    "    sim_cf['month'] = sim_cf['month'].astype(int)\n",
    "    sim_cf['year'] = sim_cf['year'].astype(int)\n",
    "    obs_cf['ID'] = obs_cf['ID'].astype(str)\n",
    "    obs_cf['month'] = obs_cf['month'].astype(int)\n",
    "    obs_cf['year'] = obs_cf['year'].astype(int)\n",
    "    \n",
    "    gen_cf = pd.merge(sim_cf, obs_cf, on=['ID', 'month', 'year'], how='left')\n",
    "    gen_cf = gen_cf.drop(['time'], axis=1).reset_index(drop=True)\n",
    "    \n",
    "    return gen_cf, sim_ws\n",
    "\n",
    "\n",
    "def calculate_scalar(time_res, gen_data):\n",
    "    scalar_alpha = 0.6\n",
    "    scalar_beta = 0.2\n",
    "\n",
    "    bias_data = gen_data.groupby([time_res, 'cluster', 'year'])[['obs','sim']].mean()\n",
    "    bias_data['scalar'] = (scalar_alpha * (bias_data['obs'] / bias_data['sim'])) + scalar_beta\n",
    "    bias_data = bias_data.reset_index()\n",
    "        \n",
    "    bias_data.columns = ['time_slice', 'cluster', 'year', 'obs', 'sim', 'scalar']   \n",
    "    return bias_data[['year', 'time_slice', 'cluster', 'obs', 'sim', 'scalar']]\n",
    "\n",
    "\n",
    "def cluster_turbines(num_clu, turb_info):\n",
    "    # generating the cluster labels \n",
    "    kmeans = KMeans(\n",
    "        init=\"random\",\n",
    "        n_clusters = num_clu,\n",
    "        n_init = 10,\n",
    "        max_iter = 300,\n",
    "        random_state = 42\n",
    "    )\n",
    "    \n",
    "    lat = turb_info['lat']\n",
    "    lon = turb_info['lon']\n",
    "    df = pd.DataFrame(list(zip(lat, lon)), columns =['lat', 'lon'])\n",
    "    kmeans.fit(df)\n",
    "    turb_info['cluster'] = kmeans.labels_\n",
    "    turb_info.to_csv('data/results/new_factors/clusters/clus_info_'+str(num_clu)+'.csv')\n",
    "    return turb_info\n",
    "    \n",
    "def train_data(time_res, gen_cf, clus_info):\n",
    "    \n",
    "    gen_data = gen_cf.groupby(['year',time_res,'ID'], as_index=False)[['obs','sim']].mean()\n",
    "\n",
    "    gen_data = pd.merge(gen_data, clus_info[['ID', 'cluster', 'lon', 'lat', 'capacity', 'height', 'model']], on='ID', how='left')\n",
    "    bias_data = calculate_scalar(time_res, gen_data)\n",
    "\n",
    "    return bias_data\n",
    "\n",
    "\n",
    "    \n",
    "def simulate_wind_speed(reanal_data, turb_info):\n",
    "    reanal_data = reanal_data.assign_coords(\n",
    "        height=('height', turb_info['height'].unique()))\n",
    "    \n",
    "    # calculating wind speed from reanalysis dataset variables\n",
    "    ws = reanal_data.wnd100m * (np.log(reanal_data.height/ reanal_data.roughness) / np.log(100 / reanal_data.roughness))\n",
    "    \n",
    "    # creating coordinates to spatially interpolate to\n",
    "    lat =  xr.DataArray(turb_info['lat'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "    lon =  xr.DataArray(turb_info['lon'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "    height =  xr.DataArray(turb_info['height'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "\n",
    "    # spatial interpolating to turbine positions\n",
    "    sim_ws = ws.interp(\n",
    "            lon=lon, lat=lat, height=height,\n",
    "            kwargs={\"fill_value\": None})\n",
    "    \n",
    "    return sim_ws\n",
    "\n",
    "def speed_to_power(column, powerCurveFile, turb_info):\n",
    "    x = powerCurveFile['data$speed']\n",
    "    turb_name = turb_info.loc[turb_info['ID'] == column.name, 'model']           \n",
    "    y = powerCurveFile[turb_name].to_numpy().flatten()\n",
    "    f = interpolate.Akima1DInterpolator(x, y)\n",
    "    return f(column)\n",
    "\n",
    "\n",
    "def simulate_wind_train(sim_ws, turb_info, powerCurveFile, scalar=1, offset=0): \n",
    "    \n",
    "    # calculating wind speed from reanalysis data\n",
    "    sim_ws['ws'] = (sim_ws['ws'] * scalar) + offset\n",
    "    sim_ws = sim_ws.pivot(index=['time'], columns='ID', values='ws')\n",
    "    sim_cf = sim_ws.apply(speed_to_power, args=(powerCurveFile, turb_info), axis=0)\n",
    "    \n",
    "    return sim_cf\n",
    "\n",
    "\n",
    "def find_offset(row, turb_info, sim_ws, powerCurveFile, time_res):\n",
    "\n",
    "    # decide our initial search step size\n",
    "    stepSize = -0.64\n",
    "    if (row.sim > row.obs):\n",
    "        stepSize = 0.64\n",
    "        \n",
    "    # start_time = time.time()\n",
    "    myOffset = 0\n",
    "    while np.abs(stepSize) > 0.002: # Stop when step-size is smaller than our power curve's resolution\n",
    "        myOffset += stepSize # If we are still far from energytarget, increase stepsize\n",
    "        \n",
    "        # calculate the mean simulated CF using the new offset\n",
    "        sim_cf = simulate_wind_train(\n",
    "            sim_ws.loc[(sim_ws['year'] == row.year) & (sim_ws[time_res] == row.time_slice) & (sim_ws['cluster'] == row.cluster)],\n",
    "            turb_info.loc[turb_info['cluster'] == row.cluster],\n",
    "            powerCurveFile, \n",
    "            row.scalar, \n",
    "            myOffset\n",
    "        )\n",
    "\n",
    "        mean_cf = np.mean(sim_cf)\n",
    "        \n",
    "        # if we have overshot our target, then repeat, searching the other direction\n",
    "        # ((guess < target & sign(step) < 0) | (guess > target & sign(step) > 0))\n",
    "        if mean_cf != 0:\n",
    "            sim = mean_cf\n",
    "            if np.sign(sim - row.obs) == np.sign(stepSize):\n",
    "                stepSize = -stepSize / 2\n",
    "            # If we have reached unreasonable places, stop\n",
    "            if myOffset < -20 or myOffset > 20:\n",
    "                break\n",
    "        elif mean_cf == 0:\n",
    "            myOffset = 0\n",
    "            break\n",
    "\n",
    "    # end_time = time.time()\n",
    "    # elapsed_time = end_time - start_time\n",
    "    # print(\"While loop took: \", elapsed_time)\n",
    "    return myOffset\n",
    "\n",
    "year_star = 2015 # start year of training period\n",
    "year_end = 2019 # end year of training period\n",
    "year_test = 2020 # year you wish to receive a time series for\n",
    "\n",
    "powerCurveFileLoc = 'data/turbine_info/Wind Turbine Power Curves.csv'\n",
    "powerCurveFile = pd.read_csv(powerCurveFileLoc)\n",
    "\n",
    "train_era5 = prep_era5(True)\n",
    "train_obs_cf, train_turb_info = prep_obs(\"DK\", year_star, year_end)\n",
    "train_gen_cf, train_all_sim_ws  = match_generation_data(train_era5, train_obs_cf, train_turb_info, powerCurveFile)\n",
    "cluster_list = [5]\n",
    "time_res_list = ['year_res', 'season', 'bimonth', 'month'] \n",
    "\n",
    "# cluster_list = [1,2]\n",
    "# time_res_list = ['yearly', 'season', 'bimonth', 'month']\n",
    "\n",
    "for num_clu in cluster_list:\n",
    "    train_clus_info = cluster_turbines(num_clu, train_turb_info)\n",
    "    train_sim_ws = pd.merge(train_all_sim_ws, train_clus_info[['ID', 'cluster', 'lon', 'lat', 'capacity', 'height', 'model']], on='ID', how='left')\n",
    "\n",
    "    for time_res in time_res_list:\n",
    "        print(\"Train for \", num_clu, \" clusters with time resolution: \", time_res, \" is taking place.\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        bias_data = train_data(time_res, train_gen_cf, train_clus_info)\n",
    "        ddf = dd.from_pandas(bias_data, npartitions=40)\n",
    "        \n",
    "        def find_offset_parallel(df):\n",
    "            return df.apply(find_offset, args=(train_clus_info, train_sim_ws, powerCurveFile, time_res), axis=1)\n",
    "            \n",
    "        ddf[\"offset\"] = ddf.map_partitions(find_offset_parallel, meta=('offset', 'float'))\n",
    "        ddf.to_csv('data/results/new_factors/'+time_res+'_'+str(num_clu)+'.csv', single_file=True, compute_kwargs={'scheduler':'processes'})\n",
    "        \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(\"Results completed and saved. Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91433177-c358-4995-abbb-5bf2ed4a85a1",
   "metadata": {},
   "source": [
    "## Trying to improve .apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3617c1-38ea-4401-915d-0a8cc0ad81e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_res = 'season'\n",
    "num_clu = 10\n",
    "test_gen = gen_data.loc[(gen_data['year'] == 2015) & (gen_data[time_res] == 'autumn') & (gen_data['cluster'] == num_clu-1)]\n",
    "test_era5 = train_era5.sel(\n",
    "                    time=np.logical_and(\n",
    "                    train_era5.time.dt.year == 2015, \n",
    "                    train_era5.time.dt.month.isin([9,10,11])\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63ce6f5-dc69-46ff-9634-639c51d19992",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cor_ws = simulate_wind_train(test_era5, test_gen, powerCurveFile, 0.8, 0)\n",
    "# row = bias_data.iloc[0,:]\n",
    "# find_offset(row, test_gen, test_era5, powerCurveFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a33afb-d168-47bd-a5b9-e3418fef6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cor_cf = cor_ws.copy()\n",
    "# x = powerCurveFile['data$speed']\n",
    "# for i in range(0, len(cor_cf.columns)):          \n",
    "#     speed_single = cor_cf.iloc[:,i]\n",
    "#     turb_name = test_gen.loc[test_gen['ID'] == speed_single.name, 'model']           \n",
    "#     y = powerCurveFile[turb_name].to_numpy().flatten()\n",
    "#     f = interpolate.Akima1DInterpolator(x, y)\n",
    "#     cor_cf.iloc[:,i] = f(speed_single)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ba4ada-6c77-4e8b-9fbe-85fb26554f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def to_power(column, powerCurveFile, test_gen):\n",
    "#     x = powerCurveFile['data$speed']\n",
    "#     turb_name = test_gen.loc[test_gen['ID'] == column.name, 'model']           \n",
    "#     y = powerCurveFile[turb_name].to_numpy().flatten()\n",
    "#     f = interpolate.Akima1DInterpolator(x, y)\n",
    "#     return f(column)\n",
    "\n",
    "# cor_ws.apply(to_power, args=(powerCurveFile, test_gen), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd23c7e-0d01-4046-9ecf-68f410822c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f1(column, turb_info):\n",
    "#     turb_name = turb_info.loc[turb_info['ID'] == column.columns, 'model']\n",
    "#     x = powerCurveFile['data$speed']\n",
    "#     y = powerCurveFile[turb_name]\n",
    "#     f = interpolate.Akima1DInterpolator(x, y)\n",
    "#     return df.asign(f(column))\n",
    "\n",
    "# def f2(df):\n",
    "#     return df.apply(f1, args=(turb_info_test), axis=0)\n",
    "\n",
    "# try_cf = unc_ws.iloc[:,:1]\n",
    "# ddf = dd.from_pandas(unc_ws.iloc[:,1:], npartitions=40)\n",
    "# p = ddf.map_partitions(f2, meta=(None, 'f8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022789f-3efa-4edb-aec8-5a78ae6155c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# p.compute(scheduler='processes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
