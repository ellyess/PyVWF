{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49835b69-bddd-4494-b8fa-50471618ed14",
   "metadata": {},
   "source": [
    "# Training method\n",
    "\n",
    "This training method calculates BC factors at an individual turbine level, which is essentially the highest resolution then is averaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69e2c5c2-7aec-4988-a453-393adf45f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import dask.dataframe as dd\n",
    "from scipy import interpolate\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "from calendar import monthrange\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from vwf.simulation import simulate_wind\n",
    "from vwf.preprocessing import (\n",
    "    prep_era5,\n",
    "    prep_obs,\n",
    "    prep_obs_test,\n",
    "    prep_merra2_method_1\n",
    ")\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4e5926-390e-429e-8f83-8c3cc896198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_times(data):\n",
    "    data['year'] = pd.DatetimeIndex(data['time']).year\n",
    "    data['month'] = pd.DatetimeIndex(data['time']).month\n",
    "    data.insert(1, 'year', data.pop('year'))\n",
    "    data.insert(2, 'month', data.pop('month'))\n",
    "    return data\n",
    "\n",
    "def simulate_wind_speed(reanal_data, turb_info):\n",
    "    reanal_data = reanal_data.assign_coords(\n",
    "        height=('height', turb_info['height'].unique()))\n",
    "    \n",
    "    # calculating wind speed from reanalysis dataset variables\n",
    "    ws = reanal_data.wnd100m * (np.log(reanal_data.height/ reanal_data.roughness) / np.log(100 / reanal_data.roughness))\n",
    "    \n",
    "    # creating coordinates to spatially interpolate to\n",
    "    lat =  xr.DataArray(turb_info['lat'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "    lon =  xr.DataArray(turb_info['lon'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "    height =  xr.DataArray(turb_info['height'], dims='turbine', coords={'turbine':turb_info['ID']})\n",
    "\n",
    "    # spatial interpolating to turbine positions\n",
    "    sim_ws = ws.interp(\n",
    "            x=lon, y=lat, height=height,\n",
    "            kwargs={\"fill_value\": None})\n",
    "    \n",
    "    return sim_ws\n",
    "    \n",
    "# def speed_to_power(sim_ws, turb_info, powerCurveFile): \n",
    "#     # identifying the model assigned to this turbine ID to access the power curve\n",
    "#     # and covert the speed into power\n",
    "#     x = powerCurveFile['data$speed']\n",
    "#     turb_name = turb_info.loc[turb_info['ID'] == sim_ws.turbine.data, 'model']\n",
    "#     y = powerCurveFile[turb_name].to_numpy().flatten()\n",
    "#     f = interpolate.Akima1DInterpolator(x, y)\n",
    "#     return f(sim_ws.data)\n",
    "\n",
    "\n",
    "def simulate_wind_train(turb_info, reanal_data, powerCurveFile, *args):\n",
    "    scalar, offset = args\n",
    "\n",
    "    # simulate wind speed and apply bias correction factors\n",
    "    sim_ws = simulate_wind_speed(reanal_data, turb_info)\n",
    "    sim_ws = (sim_ws * scalar) + offset\n",
    "    sim_ws = sim_ws.where(raw_ws > 0 , 0)\n",
    "    sim_ws = sim_ws.where(raw_ws < 40 , 40)\n",
    "\n",
    "    # converting wind speed to power, by inteporlating the manufacturer power curve\n",
    "    x = powerCurveFile['data$speed']\n",
    "    turb_name = turb_info.loc[turb_info['ID'] == sim_ws.turbine.data, 'model']\n",
    "    y = powerCurveFile[turb_name].to_numpy().flatten()\n",
    "    f = interpolate.Akima1DInterpolator(x, y)\n",
    "    \n",
    "    sim_cf = f(sim_ws.data)\n",
    "    \n",
    "    return np.mean(sim_cf)\n",
    "\n",
    "\n",
    "def find_offset(row,turb_info,reanal_data,powerCurveFile):\n",
    "    myOffset = 0\n",
    "    \n",
    "    # decide our initial search step size\n",
    "    stepSize = -0.64\n",
    "    if (row.sim > row.obs):\n",
    "        stepSize = 0.64\n",
    "        \n",
    "    # Stop when step-size is smaller than our power curve's resolution\n",
    "    while np.abs(stepSize) > 0.002:\n",
    "        # If we are still far from energytarget, increase stepsize\n",
    "        myOffset += stepSize\n",
    "        \n",
    "        # calculate the mean simulated CF using the new offset\n",
    "        mean_cf = simulate_wind_train(\n",
    "            turb_info[turb_info[\"ID\"]==row.ID], \n",
    "            reanal_data.sel(time=slice(str(row.year)+'-'+str(row.month)+'-01', str(row.year)+'-'+str(row.month)+'-'+str(monthrange(row.year, row.month)[1]))), \n",
    "            powerCurveFile, \n",
    "            row.scalar, \n",
    "            myOffset\n",
    "        )\n",
    "\n",
    "        # if we have overshot our target, then repeat, searching the other direction\n",
    "        # ((guess < target & sign(step) < 0) | (guess > target & sign(step) > 0))\n",
    "        if mean_cf != 0:\n",
    "            sim = mean_cf\n",
    "            if np.sign(sim - row.obs) == np.sign(stepSize):\n",
    "                stepSize = -stepSize / 2\n",
    "            # If we have reached unreasonable places, stop\n",
    "            if myOffset < -20 or myOffset > 20:\n",
    "                break\n",
    "        elif mean_cf == 0:\n",
    "            myOffset = 0\n",
    "            break\n",
    "    \n",
    "    return myOffset\n",
    "\n",
    "\n",
    "#### processing BCF\n",
    "def format_bc_factors(num_clu, turb_info):\n",
    "    # cluster factors first\n",
    "    lat = turb_info['lat']\n",
    "    lon = turb_info['lon']\n",
    "    df = pd.DataFrame(list(zip(lat, lon)),\n",
    "                    columns =['lat', 'lon'])\n",
    "\n",
    "    # create kmeans model/object\n",
    "    kmeans = KMeans(\n",
    "        init=\"random\",\n",
    "        n_clusters = num_clu,\n",
    "        n_init = 10,\n",
    "        max_iter = 300,\n",
    "        random_state = 42\n",
    "    )\n",
    "    kmeans.fit(df)\n",
    "    turb_info['cluster'] = kmeans.labels_\n",
    "    df = pd.read_csv('data/turbine_info/all_bias_results.csv')\n",
    "    turb_info['ID'] = turb_info['ID'].astype(str)\n",
    "    df['ID'] = df['ID'].astype(str)\n",
    "    df = pd.merge(df[['ID','year','month','scalar','offset']], turb_info[['cluster', 'ID']],  how='left', on='ID')\n",
    "    df = df.groupby(['cluster', 'month'], as_index=False).agg({'scalar': 'mean', 'offset': 'mean'})\n",
    "    \n",
    "    sea = []\n",
    "    for i in range(len(df)):\n",
    "        if (df.month[i] == 3) or (df.month[i] == 4) or (df.month[i] == 5):\n",
    "            sea.append('spring')\n",
    "        if (df.month[i] == 8) or (df.month[i] == 6) or (df.month[i] == 7):\n",
    "            sea.append('summ')\n",
    "        if (df.month[i] == 11) or (df.month[i] == 9) or (df.month[i] == 10):\n",
    "            sea.append('autum')\n",
    "        if (df.month[i] == 12) or (df.month[i] == 1) or (df.month[i] == 2):\n",
    "            sea.append('wint')\n",
    "    df['season'] = sea\n",
    "    \n",
    "    # ADD IN A COLUMN REPRESENTING THE BI-MONTHLY DIVISION\n",
    "    two = []\n",
    "    for i in range(len(df)):\n",
    "        if (df.month[i] == 1) or (df.month[i] == 2):\n",
    "            two.append('01')\n",
    "        if (df.month[i] == 3) or (df.month[i] == 4):\n",
    "            two.append('02')\n",
    "        if (df.month[i] == 5) or (df.month[i] == 6):\n",
    "            two.append('03')\n",
    "        if (df.month[i] == 7) or (df.month[i] == 8):\n",
    "            two.append('04')\n",
    "        if (df.month[i] == 9) or (df.month[i] == 10):\n",
    "            two.append('05')\n",
    "        if (df.month[i] == 11) or (df.month[i] == 12):\n",
    "            two.append('06')\n",
    "    df['two_month'] = two\n",
    "\n",
    "    return df, turb_info\n",
    "\n",
    "\n",
    "def closest_cluster(clus_info, turb_info):\n",
    "    \"\"\"\n",
    "    Assign turbines not found in training data to closest cluster.\n",
    "    \"\"\"\n",
    "    # making sure ID column dtype is same   \n",
    "    clus_info['ID'] = clus_info['ID'].astype(str)\n",
    "    turb_info['ID'] = turb_info['ID'].astype(str)\n",
    "    \n",
    "    avg = clus_info.groupby(['cluster'], as_index=False)[['lat','lon']].mean()\n",
    "    turb_info = pd.DataFrame.merge(clus_info[['ID','cluster']], turb_info, on='ID', how='right')\n",
    "\n",
    "    for i in range(len(turb_info)):\n",
    "        if np.isnan(turb_info.cluster[i]) == True:\n",
    "            # Find the cluster center closest to the new turbine\n",
    "            # - find smallest distance between the new turbine and cluster centers\n",
    "            indx = np.argmin(np.sqrt((avg.lat.values - turb_info.lat[i])**2 + (avg.lon.values - turb_info.lon[i])**2))\n",
    "            turb_info.cluster[i] = avg.cluster[indx]\n",
    "\n",
    "    turb_info = turb_info.reset_index(drop=True)\n",
    "\n",
    "    return turb_info\n",
    "\n",
    "\n",
    "#### TEST\n",
    "def speed_to_power(sim_ws, turb_info, powerCurveFile): \n",
    "    sim_cf = sim_ws.copy()\n",
    "    \n",
    "    x = powerCurveFile['data$speed']\n",
    "    for i in range(2, len(sim_cf.columns)+1):          \n",
    "        speed_single = sim_cf.iloc[:,i-1]\n",
    "        turb_name = turb_info.loc[turb_info['ID'] == speed_single.name, 'model']           \n",
    "        y = powerCurveFile[turb_name].to_numpy().flatten()\n",
    "        f = interpolate.Akima1DInterpolator(x, y)\n",
    "        sim_cf.iloc[:,i-1] = f(speed_single)\n",
    "    return sim_cf\n",
    "\n",
    "def simulate_wind_test(reanal_data, turb_info, powerCurveFile, time_res='month',train=False, bias_correct=False, *args):\n",
    "\n",
    "    sim_ws = simulate_wind_speed(reanal_data, turb_info)\n",
    "    unc_ws = sim_ws.to_pandas().reset_index()\n",
    "\n",
    "    if bias_correct == False:\n",
    "        unc_cf = speed_to_power(unc_ws, turb_info[['ID','model']], powerCurveFile)\n",
    "        return unc_ws, unc_cf\n",
    "        \n",
    "    else:\n",
    "        bc_factors = args[0] # reading in bias correction factors\n",
    "        unc_ws = unc_ws.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "            var_name=\"ID\", \n",
    "            value_name=\"ws\")\n",
    "        unc_ws = add_times(unc_ws)\n",
    "\n",
    "        \n",
    "        # unc_ws['ID'] = unc_ws['ID'].astype(str)\n",
    "        # turb_info['ID'] = turb_info['ID'].astype(str)\n",
    "        # bc_factors['ID'] = bc_factors['ID'].astype(str)\n",
    "        \n",
    "        # matching the correct resolution bias correction factors\n",
    "        unc_ws = pd.merge(unc_ws, turb_info[['ID', 'cluster']], on='ID', how='left')  \n",
    "        if time_res == 'year':\n",
    "            time_factors = bc_factors.groupby(['cluster'], as_index=False).agg({'scalar': 'mean', 'offset': 'mean'})\n",
    "            unc_ws = pd.merge(unc_ws, time_factors[['cluster','scalar', 'offset']],  how='left', on=['cluster'])\n",
    "        \n",
    "        else:        \n",
    "            unc_ws = pd.merge(unc_ws, bc_factors[['cluster', 'month','two_month','season']],  how='left', on=['cluster', 'month'])\n",
    "            time_factors = bc_factors.groupby(['cluster',time_res], as_index=False).agg({'scalar': 'mean', 'offset': 'mean'})\n",
    "            unc_ws = pd.merge(unc_ws, time_factors[['cluster', time_res, 'scalar', 'offset']],  how='left', on=['cluster', time_res])\n",
    "\n",
    "        # applying the bias correction factors to wind speed\n",
    "        # cor_ws['speed'] = (unc_ws.ws + unc_ws.offset) * gen_speed.scalar # equation 1 \n",
    "        unc_ws['ws'] = (unc_ws.ws * unc_ws.scalar) + unc_ws.offset # equation 2\n",
    "        cor_ws = unc_ws.pivot(index=['time'], columns='ID', values='ws').reset_index()\n",
    "        cor_cf = speed_to_power(cor_ws,turb_info[['ID','model']], powerCurveFile)\n",
    "        return cor_ws, cor_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a675601-65e9-4c34-be6d-8eab7c050d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of turbines before preprocessing:  5682\n",
      "Number of turbines used in training:  3712\n"
     ]
    }
   ],
   "source": [
    "year_star = 2015 # start year of training period\n",
    "year_end = 2019 # end year of training period\n",
    "year_test = 2020 # year you wish to receive a time series for\n",
    "\n",
    "powerCurveFileLoc = 'data/turbine_info/Wind Turbine Power Curves.csv'\n",
    "powerCurveFile = pd.read_csv(powerCurveFileLoc)\n",
    "\n",
    "era5_train = prep_era5(True)\n",
    "obs_cf, turb_info_train = prep_obs(\"DK\", year_star, year_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82df64-40af-433d-89b0-9c1b62515d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep training data\n",
    "unc_ws_train, unc_cf_train = simulate_wind(era5_train, turb_info_train, powerCurveFile)\n",
    "unc_cf = unc_cf_train.groupby(pd.Grouper(key='time',freq='M')).mean().reset_index()\n",
    "unc_cf = unc_cf.melt(id_vars=[\"time\"], \n",
    "                var_name=\"ID\", \n",
    "                value_name=\"sim\")\n",
    "unc_cf = add_times(unc_cf)\n",
    "unc_cf['ID'] = unc_cf['ID'].astype(str)\n",
    "unc_cf['month'] = unc_cf['month'].astype(int)\n",
    "unc_cf['year'] = unc_cf['year'].astype(int)\n",
    "obs_cf2 = obs_cf\n",
    "obs_cf2.columns = ['ID','1','2','3','4','5','6','7','8','9','10','11','12','year']\n",
    "obs_cf2 = obs_cf2.melt(id_vars=[\"ID\", \"year\"], \n",
    "                var_name=\"month\", \n",
    "                value_name=\"obs\")\n",
    "obs_cf2['ID'] = obs_cf2['ID'].astype(str)\n",
    "obs_cf2['month'] = obs_cf2['month'].astype(int)\n",
    "obs_cf2['year'] = obs_cf2['year'].astype(int)\n",
    "train_cf = pd.merge(unc_cf, obs_cf2, on=['ID','month', \"year\"], how='left')\n",
    "scalar_alpha = 0.6\n",
    "scalar_beta = 0.2\n",
    "train_cf['scalar'] = (scalar_alpha * (train_cf['obs']/train_cf['sim'])) + scalar_beta\n",
    "train_cf = train_cf.drop(['time'], axis=1).reset_index(drop=True)\n",
    "\n",
    "train_cf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6217531f-2f7b-4c2c-b9ed-8e85f85f059f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def find_offset_parallel(df):\n",
    "    return df.apply(find_offset, args=(turb_info_train,era5_train,powerCurveFile), axis=1)\n",
    "\n",
    "ddf = dd.from_pandas(train_cf, npartitions=40)\n",
    "ddf[\"offset\"] = ddf.map_partitions(find_offset_parallel, meta=('offset', 'f8'))\n",
    "ddf.to_csv('data/turbine_info/all_bias_results.csv', single_file=True, compute_kwargs={'scheduler':'processes'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a8978-fbde-4cd8-a6c8-53881dd56be3",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1d7f0ca-ba59-4aba-80cd-5e651b6d1410",
   "metadata": {},
   "outputs": [],
   "source": [
    "era5_test = prep_era5()\n",
    "turb_info_test = prep_obs_test(\"DK\", year_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b618272b-1d49-4389-8fd7-184765f27de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for  2020  using  3000  clusters with time resolution:  year  is taking place.\n",
      "Results completed and saved. Elapsed time: 11.21 seconds\n",
      " \n",
      "Test for  2020  using  3000  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 11.86 seconds\n",
      " \n",
      "Test for  2020  using  3000  clusters with time resolution:  two_month  is taking place.\n",
      "Results completed and saved. Elapsed time: 11.26 seconds\n",
      " \n",
      "Test for  2020  using  3000  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 11.29 seconds\n",
      " \n",
      "Test for  2020  using  3711  clusters with time resolution:  year  is taking place.\n",
      "Results completed and saved. Elapsed time: 11.67 seconds\n",
      " \n",
      "Test for  2020  using  3711  clusters with time resolution:  season  is taking place.\n",
      "Results completed and saved. Elapsed time: 12.20 seconds\n",
      " \n",
      "Test for  2020  using  3711  clusters with time resolution:  two_month  is taking place.\n",
      "Results completed and saved. Elapsed time: 12.04 seconds\n",
      " \n",
      "Test for  2020  using  3711  clusters with time resolution:  month  is taking place.\n",
      "Results completed and saved. Elapsed time: 12.16 seconds\n",
      " \n"
     ]
    }
   ],
   "source": [
    "time_res_list = ['year', 'season', 'two_month', 'month'] \n",
    "cluster_list = [3000,3711]#[1,2,3,4,5,6,8,10,15,20,30,50,100,150,200,300,400,500,750,1000,1500,2000,3000,3711] # [1,2,3,5,10,15,20,30,50,100,150,200,300,400]\n",
    "\n",
    "unc_ws, unc_cf = simulate_wind_test(era5_test, turb_info_test, powerCurveFile)\n",
    "unc_ws.to_csv('data/results/new/'+str(year_test)+'_unc_ws.csv', index = None)\n",
    "unc_cf.to_csv('data/results/new/'+str(year_test)+'_unc_cf.csv', index = None)\n",
    "\n",
    "for num_clu, time_res in itertools.product(cluster_list, time_res_list):\n",
    "    # producing corrected results\n",
    "    print(\"Test for \", year_test, \" using \", num_clu, \" clusters with time resolution: \", time_res, \" is taking place.\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    bc_factors, clus_info = format_bc_factors(num_clu, turb_info_train)\n",
    "    clus_info_test = closest_cluster(clus_info, turb_info_test)\n",
    "\n",
    "    cor_ws, cor_cf = simulate_wind_test(era5_test, clus_info_test, powerCurveFile, time_res, False, True, bc_factors)\n",
    "    \n",
    "    cor_ws.to_csv('data/results/new/'+str(year_test)+'_'+time_res+'_'+str(num_clu)+'_cor_ws.csv', index = None)\n",
    "    cor_cf.to_csv('data/results/new/'+str(year_test)+'_'+time_res+'_'+str(num_clu)+'_cor_cf.csv', index = None)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Results completed and saved. Elapsed time: {:.2f} seconds\".format(elapsed_time))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5b822b-743c-4b6f-b2f3-73ddd3c8aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cor = cor_cf\n",
    "# cor = cor.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "#         var_name=\"ID\", \n",
    "#         value_name=\"cor\")\n",
    "# cor_month = cor.groupby([pd.Grouper(key='time',freq='M')])['cor'].mean().reset_index()\n",
    "    \n",
    "# unc = unc_cf\n",
    "# unc = unc.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "#     var_name=\"ID\", \n",
    "#     value_name=\"unc\")\n",
    "# unc_month = unc.groupby([pd.Grouper(key='time',freq='M')])['unc'].mean().reset_index()\n",
    "\n",
    "# obs = pd.read_csv('data/wind_data/DK/obs_cf_test.csv', parse_dates=['time'])\n",
    "# obs = obs.melt(id_vars=[\"time\"], # adding in turbine ID for merging\n",
    "#     var_name=\"ID\", \n",
    "#     value_name=\"obs\")\n",
    "# obs_month = obs.groupby([pd.Grouper(key='time',freq='M')])['obs'].mean().reset_index()\n",
    "\n",
    "# cf_month = obs_month.merge(unc_month,on=['time']).merge(cor_month,on=['time'])\n",
    "# cf_month.columns = [\"time\", 'obs', 'unc', \"cor\"]\n",
    "# cf_month[\"time\"] = cf_month[\"time\"].dt.month\n",
    "# cf_month = cf_month.melt(id_vars=[\"time\"],\n",
    "#             var_name=\"model\", \n",
    "#             value_name=\"CF\")\n",
    "\n",
    "\n",
    "# sns.lineplot(\n",
    "#     data = cf_month,\n",
    "#     x=\"time\",\n",
    "#     y=\"CF\",\n",
    "#     hue=\"model\",\n",
    "#     style=\"model\",\n",
    "#     legend = True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c6470-5f76-4717-8ed3-8affa412a3ca",
   "metadata": {},
   "source": [
    "### Attempted to improve speed of speed to power with dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd23c7e-0d01-4046-9ecf-68f410822c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f1(column, turb_info):\n",
    "#     turb_name = turb_info.loc[turb_info['ID'] == column.columns, 'model']\n",
    "#     x = powerCurveFile['data$speed']\n",
    "#     y = powerCurveFile[turb_name]\n",
    "#     f = interpolate.Akima1DInterpolator(x, y)\n",
    "#     return df.asign(f(column))\n",
    "\n",
    "# def f2(df):\n",
    "#     return df.apply(f1, args=(turb_info_test), axis=0)\n",
    "\n",
    "# try_cf = unc_ws.iloc[:,:1]\n",
    "# ddf = dd.from_pandas(unc_ws.iloc[:,1:], npartitions=40)\n",
    "# p = ddf.map_partitions(f2, meta=(None, 'f8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e022789f-3efa-4edb-aec8-5a78ae6155c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# p.compute(scheduler='processes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
