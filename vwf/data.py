"""
data module.

Summary
-------
Preprocessing of the data required in the model.

Data conventions
----------------
Tabular inputs are assumed to be tidy (one observation per row) unless stated otherwise.
Datetime columns are assumed to be timezone-naive UTC unless specified.

Units
-----
Wind speed: [m s^-1]; Hub height: [m]; Power: [MW]; Energy: [MWh]; Capacity factor: [-] (unless stated otherwise).

Assumptions
-----------
- ERA5/reanalysis fields are treated as representative at the chosen spatial/temporal resolution.
- Wake effects, curtailment, availability losses are not modelled unless explicitly implemented in this module.

References
----------
Add dataset and methodological references relevant to this module.
"""
import numpy as np
import pandas as pd
import geopandas as gpd
import difflib

import utm
from calendar import monthrange


import vwf.wind as wind
from vwf.datasets.era5 import (
    prep_era5,
)
from vwf.clustering import (
    cluster_turbines,
)
import vwf.correction as correction

def clean_obs_data(df, country, train=False):
    """
    Preprocess the turbines/farms found in the observed data.
    
    Prepares the observational data (obs_cf and turb_info) for the desired country
    used in the model. Converts observed power generation into observed CF and ensures
    that all turbines in the data are acceptable.

        Args:
            country (str): country code e.g. Denmark "DK"
            
        Returns:
            obs_gen (pandas.DataFrame): obs_gen which is a time series of the power generated by a turbine
            turb_info (pandas.DataFrame): consists of the turbines metadata; latitude, longitude, capacity, height, rotor diameter and model
    """

    ################################################################################
    ### conditions to remove turbines from the data    
    # cf can't be greater than 100%
    df['cf_max'] = df[df.columns[df.columns.str.startswith('obs')]].max(axis=1)
    df = df.drop(df[df['cf_max'] > 1].index)
    df = df.drop('cf_max', axis=1)
    
    # case exists for Denmark solely, develop a method to consider the weight of missing data
    # remove any turbines that have cf of 0 at any point
    if (train) & (country == 'DK'):
        df['cf_min'] = df[df.columns[df.columns.str.startswith('obs')]].min(axis=1)
        df = df.drop(df[df['cf_min'] <= 0.01].index)
        df = df.drop('cf_min', axis=1)
    
    # turn 0 into nan to not be considered in groupby functions
    df = df.replace(0, np.nan)
    
    # turbine should atleast have a cf of atleast 1%
    df['cf_mean'] = df[df.columns[df.columns.str.startswith('obs')]].mean(axis=1)
    df = df.drop(df[df['cf_mean'] <= 0.01].index)
    df = df.drop(['cf_mean'], axis=1)

    return df
    
    
def load_power_curves():
    """
    Load power curves.
    """
    file_loc = 'input/power_curves.csv'
    df = pd.read_csv(file_loc)
    return df
    

def train_set(
    country,
    calc_z0,
    mode='all', 
    year_test=None, 
    add_nan=None, 
    interp_nan=None, 
    fix_turb=None
    ):
    """
    Generate all the data required for training.

    Prepares the generation data, turbine information, power curves
    and reanalysis data.

        Args:
            country (str): country code e.g. Denmark "DK"
            
        Returns:
            gen_cf (pandas.DataFrame): monthly obs and sim cf which is a time series of the power generated by a turbine
            turb_info (pandas.DataFrame): consists of the turbines metadata; latitude, longitude, capacity, height, rotor diameter and model
    """
    
    obs_cf, turb_info = prep_country(country)
    
    if mode != 'all':
            turb_info = turb_info[turb_info['type'] == mode].copy()

    obs_cf = clean_obs_data(obs_cf, country, True)
    
    # further rules for the training data and formatting
    year_star = obs_cf.year.min()
    year_end = obs_cf.year.max()
    
    ################################################################################
    # turbines should exist the entire period
    obs_cf = obs_cf[obs_cf.groupby('ID').ID.transform('count') == ((year_end-year_star)+1)].reset_index(drop=True) 
    # reordering and renaming columns
    obs_cf = obs_cf[[
        'ID','year',
        'obs_1','obs_2','obs_3','obs_4','obs_5','obs_6',
        'obs_7','obs_8','obs_9','obs_10','obs_11','obs_12'
        ]]      
    obs_cf.columns = ['ID','year','1','2','3','4','5','6','7','8','9','10','11','12']
    # keeping data that has a turbine match
    obs_cf = obs_cf.loc[obs_cf['ID'].isin(turb_info['ID'])].reset_index(drop=True)
    #  changing shape of df and setting types
    obs_cf = obs_cf.melt(
        id_vars=["ID", "year"], 
        var_name="month", 
        value_name="obs"
    )
    obs_cf['month'] = obs_cf['month'].astype(int)
    obs_cf['year'] = obs_cf['year'].astype(int)
    
    ################################################################################
    ### extra functionality to modify train data
    # creates nans for sparser data
    if add_nan != None:
        obs_cf['obs'] = obs_cf['obs'].sample(frac=(1-add_nan), random_state=42)
    # interpolates nan values
    if interp_nan != None:
        obs_cf = interp_nans(obs_cf, interp_nan)
    # setting turbine model to one turbine model for research
    if fix_turb != None:
        turb_info['model'] = fix_turb 

    ################################################################################
    # keeping data that has a turbine match
    turb_info = turb_info.loc[turb_info['ID'].isin(obs_cf['ID'])].reset_index(drop=True)
    # preping era5 for train
    reanalysis = prep_era5(country, True, calc_z0)
    # loading power curve df
    power_curves = load_power_curves()
    
    ################################################################################
    ### merging the observed CF and the simulated CF for the every turbine
    # simulating turbines in the observed data
    sim_ws, sim_cf = wind.simulate_wind(reanalysis, turb_info, power_curves)
    
    # making it monthly to match the resolution of observation
    # splitting data into month and year and adding the associated time slice
    sim_cf = sim_cf.groupby(pd.Grouper(key='time',freq='ME')).mean().reset_index()
    sim_cf = sim_cf.melt(id_vars=["time"], 
                    var_name="ID", 
                    value_name="sim")
    sim_cf = add_times(sim_cf)
    sim_cf = add_time_res(sim_cf)
    sim_cf['ID'] = sim_cf['ID'].astype(str)
    obs_cf['ID'] = obs_cf['ID'].astype(str)
    
    gen_cf = pd.merge(sim_cf, obs_cf, on=['ID', 'month', 'year'], how='left')
    gen_cf = gen_cf.drop(['time'], axis=1).reset_index(drop=True)
    
    return gen_cf, turb_info, reanalysis, power_curves 
        

def val_set(country, calc_z0, mode='all', year_test=None, fix_turb=None):
    """
    Validation set preparation.

        Args:
            country (str): Country code e.g. Denmark "DK"
            calc_z0 (bool): If True calculate z0 from landuse data.
            mode (str, optional): Choose a mode running the function for onshore, offshore or all turbines. Defaults to 'all'.
            year_test (int, optional): _description_. Defaults to None.
            fix_turb (str, optional): _description_. Defaults to None.

        Returns:
            obs_cf (pandas.DataFrame): monthly obs cf which is a time series of the power generated by a turbine
            turb_info (pandas.DataFrame): consists of the turbines metadata; latitude, longitude, capacity, height, rotor diameter and model
            reanalysis (pandas.DataFrame): reanalysis data for the country and period
            power_curves (pandas.DataFrame): power curves dataframe
    """
    obs_cf, turb_info = prep_country(country, year_test)

    if mode != 'all':
            turb_info = turb_info[turb_info['type'] == mode].copy()

    if fix_turb != None:
        turb_info['model'] = fix_turb
    
    obs_cf = clean_obs_data(obs_cf, country, False)
    
    # formatting for testing
    dates = np.arange(str(year_test)+'-01', str(year_test+1)+'-01', dtype='datetime64[M]')
    cols = dates.tolist()
    obs_cf = obs_cf.drop('year', axis=1)
    obs_cf.columns = ['ID'] + cols
    obs_cf = obs_cf.loc[obs_cf['ID'].isin(turb_info['ID'])].reset_index(drop=True)
    turb_info = turb_info.loc[turb_info['ID'].isin(obs_cf['ID'])].reset_index(drop=True)
    obs_cf = obs_cf.set_index('ID').transpose().rename_axis('time').reset_index()
    
    # preping era5 for train
    reanalysis = prep_era5(country, False, calc_z0)
    
    # loading power curve df
    power_curves = load_power_curves()
    
    return obs_cf, turb_info, reanalysis, power_curves 


def cluster_train_set(gen_cf, time_res, num_clu, turb_info):
    """
    Applying desired resolution to train set and calculating scalar.
    
        Args:
            gen_cf (pandas.DataFrame): monthly obs and sim cf which is a time series of the power generated by a turbine
            time_res (str): desired temporal resolution e.g. 'month', 'bimonth', 'season', 'fixed'
            num_clu (int): number of clusters to generate
            turb_info (pandas.DataFrame): consists of the turbines metadata; latitude, longitude, capacity, height, rotor diameter and model
        
        Returns:
            train_bias_df (pandas.DataFrame): dataframe consisting of the bias correction scalars for each cluster and time period
            clus_info (pandas.DataFrame): dataframe consisting of the turbine cluster assignments and metadata 
    """

    # time average to desired temporal resolution
    gen_cf = gen_cf.groupby(['year',time_res,'ID'], as_index=False)[['obs','sim']].mean()

    # generate clusters and assign to gen data
    clus_info = cluster_turbines(num_clu, turb_info, True)
    gen_cf = pd.merge(gen_cf, clus_info[['ID', 'cluster', 'lon', 'lat', 'capacity', 'height', 'model']], on='ID', how='left')
    
    # spatial average to desired spatial resolution and calculate scalar
    train_bias_df = correction.calculate_scalar(gen_cf, time_res)

    return train_bias_df, clus_info
    

def interp_nans(df, limit):
    """
    Interpolate nans in observed data.
    """
    df = df.sort_values(['ID','year']).groupby(['ID']).apply(
        lambda group: group.interpolate(
            method='linear',
            limit=limit, 
            limit_direction='both',
    )).reset_index(drop=True).sort_values(['year','month'])
    return df
    


def add_models(df):
    """
    Assign model names to input turbines.
    
    Using our collection of models to assign power curves to observational data,
    matching is done via most similar power density, using the manufacturer if
    possible.

        Args:
            df (pandas.DataFrame): dataframe with input turbine metadata
            
        Returns:
            df (pandas.DataFrame): input df with added column called model
    """

    models = pd.read_csv('input/models.csv')
    models['model'] = models['model'].astype(pd.StringDtype())
    models['manufacturer'] = models['manufacturer'].str.lower()

    print("Total observed turbines/farms before conditions: ", len(df))
    
    df['capacity'] = df['capacity'].astype(float)
    df['diameter'] = df['diameter'].astype(float)
    df['height'] = df['height'].astype(float)
    # removing turbines that are unrealistic
    df = df.drop(df[df['height'] < 1].index).reset_index(drop=True)
    
    
    df['p_density'] = (df['capacity']*1000) / (np.pi * (df['diameter']/2)**2)
    df['capacity'] = df['capacity'].astype(int)
    df['ID'] = df['ID'].astype(str)
    
    # merging by manufacturer and power density if available
    if 'manufacturer' in df:
        df['manufacturer'] = df['manufacturer'].astype(pd.StringDtype())
        df['manufacturer'] = df['manufacturer'].str.lower()

        df = df.merge(models
            .assign(match=models['manufacturer'].apply(lambda x: difflib.get_close_matches(x, df['manufacturer'],cutoff=0.3,n=100)))
            .explode('match').drop_duplicates(),
            # .explode('manufacturer'),
            left_on=['manufacturer'], right_on=['match'],
            how="outer"
        )
        df = df.dropna(subset=['ID'])
        
        df = (
            df.assign(
                closest= np.abs(df['p_density_x'] - df['p_density_y'])
            )
            .sort_values("closest")
            .drop_duplicates(subset=["ID"], keep="first")
        )
        # allowing for better power density match if manufaturer model is not close enough
        # df['model'].where(df['closest'] < 1, np.nan, inplace=True)
        df['model'] = df['model'].where(df['closest'] < 1, np.nan)
        
        df = df.drop(['diameter_y', 'p_density_y', 'offshore','manufacturer_y', 'capacity_y','match','closest','manufacturer_x'], axis=1)

    if 'type' in df.columns:
        df.columns = ['ID','capacity','diameter','height','lon','lat','type','p_density','model']
    else:
        df.columns = ['ID','capacity','diameter','height','lon','lat','p_density','model']
        df['type'] = 'onshore'
        
    df = df[['ID','type','capacity','diameter','height','lon','lat','model', 'p_density']]
    
    # matching on closest power density with a given tolerance
    df = df.sort_values('p_density').reset_index(drop=True)
    models = models.sort_values('p_density')
    df.loc[df['model'].isna(), 'model'] = pd.merge_asof(df, models, on='p_density', direction="nearest",tolerance=100)['model_y']

    df = df.dropna(subset=['model'])
    df = df.sort_values('ID').reset_index(drop=True)
    
    return df
    
def format_bc_factors(train_bias_df, time_res):
    """
    Mean of all the bias corrections factors calculated for every cluster and time period.
    If the scalar is NA then set to unbiascorrected.
    
        Args:
            train_bias_df (pandas.DataFrame): dataframe consisting of the bias correction scalars for each cluster and time period
            time_res (str): desired temporal resolution e.g. 'month', 'bimonth', 'season', 'fixed'
        Returns:
            bc_factors (pandas.DataFrame): dataframe consisting of the mean bias correction scalars for each cluster and time period
    """
    # clean the bias data up
    train_bias_df = train_bias_df.drop(['obs','sim'], axis=1)
    train_bias_df['scalar'] = train_bias_df['scalar'].replace(0, np.nan)
    train_bias_df.columns = ['year',time_res,'cluster','scalar','offset']

    # group and calculate the mean
    bc_factors = train_bias_df.groupby(['cluster', time_res], as_index=False).agg({'scalar': 'mean', 'offset': 'mean'})
    bc_factors.loc[bc_factors['scalar'].isna(), 'offset'] = 0
    bc_factors.loc[bc_factors['scalar'].isna(), 'scalar'] = 1
    
    return bc_factors


def add_times(df):
    """
    Add columns to identify year and month.
    """
    df['year'] = pd.DatetimeIndex(df['time']).year
    df['month'] = pd.DatetimeIndex(df['time']).month
    df.insert(1, 'year', df.pop('year'))
    df.insert(2, 'month', df.pop('month'))
    df['month'] = df['month'].astype(int)
    df['year'] = df['year'].astype(int)
    return df

def add_time_res(df):
    """
    Add columns to identify time resolutions.
    """
    df.loc[df['month'] == 1, ['bimonth','season']] = ['1/6', 'winter']
    df.loc[df['month'] == 2, ['bimonth','season']] = ['1/6', 'winter']
    df.loc[df['month'] == 3, ['bimonth','season']] = ['2/6', 'spring']
    df.loc[df['month'] == 4, ['bimonth','season']] = ['2/6', 'spring']
    df.loc[df['month'] == 5, ['bimonth','season']] = ['3/6', 'spring']
    df.loc[df['month'] == 6, ['bimonth','season']] = ['3/6', 'summer']
    df.loc[df['month'] == 7, ['bimonth','season']] = ['4/6', 'summer']
    df.loc[df['month'] == 8, ['bimonth','season']] = ['4/6', 'summer']
    df.loc[df['month'] == 9, ['bimonth','season']] = ['5/6', 'autumn']
    df.loc[df['month'] == 10, ['bimonth','season']] = ['5/6', 'autumn']
    df.loc[df['month'] == 11, ['bimonth','season']] = ['6/6', 'autumn']
    df.loc[df['month'] == 12, ['bimonth','season']] = ['6/6', 'winter']
    df['fixed'] = '1/1'
    return df
    
def prep_country(country, year_test=None):
    """
    Country specific preprocessing of observational data.
    
    Prepping the relevant countries observational data into a format that can
    be managed in `prep_obs()` each country will be unique here. Produce turb_info 
    then obs_gen.

        Args:
            country (str): country code e.g. Denmark "DK"
            year_test (int, optional): year for testing data. Defaults to None.
            
        Returns:
            obs_gen (pandas.DataFrame): obs_gen which is a time series of the power generated by a turbine
            turb_info (pandas.DataFrame): consists of the turbines metadata; latitude, longitude, capacity, height, rotor diameter and model
    """
    if year_test != None:
        train = False
    else:
        train = True
        
    # for Denmark the metadata of existing turbines exist on anlaeg.xlsx,
    # sourced from Denmark Energy Agency. DK_md.csv is a tidied version.
    if country == "DK":
        # producing turb_info
        dk_md = pd.read_csv('input/country-data/DK/observations/DK_md.csv')
        # selecting relevent columns
        columns = ['Turbine identifier (GSRN)',
                'Manufacture','Capacity (kW)',
                'Rotor diameter (m)','Hub height (m)',
                'X (east) coordinate\nUTM 32 Euref89',
                'Y (north) coordinate\nUTM 32 Euref89', 
                'Type of location']
        dk_md = dk_md[columns]
        dk_md.columns = ['ID','manufacturer','capacity','diameter','height','x_east_32','y_north_32', 'type']

        # onshore or offshore
        dk_md['type'] = dk_md['type'].str.lower()
        dk_md.loc[dk_md['type'] == 'land', 'type'] = 'onshore'
        dk_md.loc[dk_md['type'] == 'hav', 'type'] = 'offshore'
        
        # convert coordinate system
        dk_md['x_east_32'] = pd.to_numeric(dk_md['x_east_32'],errors = 'coerce')
        dk_md['y_north_32'] = pd.to_numeric(dk_md['y_north_32'],errors = 'coerce')
        dk_md = dk_md.dropna(subset=['capacity', 'diameter', 'x_east_32','y_north_32']).reset_index(drop=True)
        
        def rule(row):
            """
            Rule.

                Args:
                    row (Any): TODO.
                    *args (tuple): Additional positional arguments.

                Returns:
                    None: TODO.
            """
            lat, lon = utm.to_latlon(row["x_east_32"], row["y_north_32"], 32, 'W')
            return pd.Series({"lat": lat, "lon": lon})
        
        dk_md = dk_md.merge(dk_md.apply(rule, axis=1), left_index= True, right_index= True)
        
        dk_md = dk_md[['ID','manufacturer','capacity','diameter','height','lon','lat', 'type']]
        # getting first word of manufacturer name
        dk_md['manufacturer'] = dk_md['manufacturer'].str.split(' ').str[0]
        
        turb_info = add_models(dk_md)
    
        # producing obs_gen
        # load observation data and slice the observed power for chosen years
        if train == True:
            year_star = 2015 # start year of training period
            year_end = 2019 # end year of training period
        
        # this is for testing I use year_star and end for the sake of keeping code same
        else:
            year_star = year_test 
            year_end = year_test 
            
        appended_data = []
        for i in range(year_star, year_end+1):
            data = pd.read_excel('input/country-data/DK/observations/Denmark_'+str(i)+'.xlsx')
            data = data.iloc[3:,np.r_[0:1, 3:15]] # the slicing done here is file dependent please consider this when other files are used
            data.columns = ['ID','1','2','3','4','5','6','7','8','9','10','11','12']
            data['ID'] = data['ID'].astype(str)
            data = data.reset_index(drop=True)
            data['year'] = i
            appended_data.append(data[:-1])

        obs_gen = pd.concat(appended_data).reset_index(drop=True)
        obs_gen = obs_gen.fillna(0)

    # Germany data is sourced from Iain Staffell
    elif country == "DE":
        # producing turb_info
        de_geo = pd.read_csv('input/country-data/DE/observations/geolocate.germany.csv') # contains the postcode and lat lon
        de_md = pd.read_csv('input/country-data/DE/observations/DE_md.csv') # contains turbine metda data
        
        # selecting relevent columns
        de_md = de_md[['V1','Manufacturer','kW','Rotor..m.','Tower..m.']]
        de_md.columns = ['ID','manufacturer','capacity','diameter','height']
        de_md['postcode'] = de_md['ID'].astype(str).str[:5].astype(int)
    
        # geolocating by postcode
        de_md = pd.merge(de_md, de_geo[['postcode','lon','lat']], on='postcode', how='left')
        de_md = de_md.drop(["postcode"], axis=1)
        de_md = de_md.dropna(subset=['capacity', 'diameter', 'lon', 'lat']).reset_index(drop=True)
        
        turb_info = add_models(de_md)
        
        # producing obs_gen
        if train == True:
            year_star = 2015 # start year of training period
            year_end = 2018 # end year of training period
            
        else: # this is for testing I use year_star and end for the sake of keeping code same
            year_star = year_test 
            year_end = year_test 
            
        # load observation data and slice the observed power for chosen years
        de_data = pd.read_csv('input/country-data/DE/observations/DE_data.csv')
        de_data = de_data.loc[(de_data["Year"] >= year_star) & (de_data["Year"] <= year_end)].drop(['Downtime'], axis=1).reset_index(drop=True)   
        de_data.columns = ['ID','year','month','output']
        de_data = de_data.dropna(subset=['ID', 'year', 'month'])
        obs_gen = de_data.pivot(index=['ID','year'], columns='month', values='output').reset_index()
        obs_gen = obs_gen.fillna(0)
    
    # UK data and code was sourced from https://doi.org/10.48550/arXiv.2511.04781
    elif country == "UK":
        uk_md = pd.read_csv('input/country-data/UK/observations/uk_md.csv')
        turb_info = add_models(uk_md)

        # producing obs_gen
        if train == True:
            year_star = 2015  # start year of training period
            year_end = 2018  # end year of training period
            
        # this is for testing I use year_star and end for the sake of keeping code same
        else:
            year_star = year_test
            year_end = year_test
            
        obs_gen = pd.read_csv('input/country-data/UK/observations/ukobs.csv')
    
    elif country == "FR":
        fr_md = gpd.read_file("input/country-data/fr/fr_turb_info.csv")
        fr_md = fr_md.loc[fr_md['statut_parc'] == 'Autorisé'].reset_index(drop=True)
        fr_md = fr_md.loc[
            :, [
            "id_aerogenerateur", 
            "puissance_mw", 
            "diametre_rotor",
            "hauteur_mat_nacelle",

            "constructeur",
            "x_aerogenerateur",
            "y_aerogenerateur",
            "epsg"
            ]]
        fr_md.columns = [
            "ID",
            "capacity",
            "diameter",
            "height",
            "manufacturer",
            "x",
            "y",
            "epsg"
            ]

        points_gdf = gpd.GeoDataFrame(
            fr_md[["ID","capacity","diameter","height","manufacturer"]],
            geometry=gpd.points_from_xy(fr_md.x, fr_md.y, crs=fr_md.epsg.iloc[0])
            ).to_crs(epsg=4326)

        points_gdf['capacity'] = points_gdf['capacity'].astype(float) * 1e3  # MW to kW
        points_gdf['lon'] = points_gdf.geometry.x  
        points_gdf['lat'] = points_gdf.geometry.y
        points_gdf = points_gdf.drop(columns='geometry')
        turb_info = add_models(points_gdf)

        # producing obs_gen
        if train == True:
            year_star = 2015  # start year of training period
            year_end = 2018  # end year of training period
            
        # this is for testing I use year_star and end for the sake of keeping code same
        else:
            year_star = year_test
            year_end = year_test
    
        # fr_data = pd.read_csv("input/country-data/FR/observations/Wind_power_generation_in_France.csv")
        # fr_data.columns = ["date", "filter", "output", "nature"]

        # fr_data = fr_data.loc[fr_data['filter'].str.contains('evolution')].reset_index(drop=True)
        # # fr_data["type"] = fr_data["filter"].str.split(" ").str[0].str.lower()
        # fr_data["output"] = fr_data["output"].replace(',', '.', regex=True)
        # fr_data = fr_data.drop(columns=["nature","filter"])
        # fr_data['date'] = pd.to_datetime(fr_data['date'])
        # fr_data['output'] = pd.to_numeric(fr_data['output'])
        # # convert from terawatts to kilowatts
        # fr_data['output'] = fr_data['output'] * 1e9
        # fr_data['year'] = fr_data['date'].dt.year.astype(int)
        # fr_data['month'] = fr_data['date'].dt.month.astype(int)
        # fr_data = fr_data.drop(columns=['date'])

        # fr_data = fr_data.fillna(0).groupby(['year','month'])['output'].sum().reset_index()
        # # fr_data['type'] = 'onshore'

        # turb_info["ratio"] = turb_info['capacity'] / turb_info['capacity'].sum()

        # fr_data = fr_data.merge(turb_info[['ID', 'ratio']], how="cross")
        # fr_data["output"] = fr_data["output"] * fr_data["ratio"]
        # fr_data = fr_data.dropna(subset=['ID', 'year', 'month'])
        # fr_data = fr_data.loc[(fr_data["year"] >= year_star) & (fr_data["year"] <= year_end)].reset_index(drop=True)   
        # obs_gen = fr_data.pivot(index=['ID','year'], columns='month', values='output').reset_index()
        
        # ALTERNATIVE FR GENERATION DATA
        # https://ec.europa.eu/eurostat/databrowser/view/nrg_cb_pem__custom_19402431/default/table
        ns_data = pd.read_csv("input/country-data/northsea_country_generation.csv")
        ns_data = ns_data.loc[
            :, [
            "Standard international energy product classification (SIEC)", 
            "TIME_PERIOD", 
            "OBS_VALUE",
            "geo",
            ]]
        ns_data.columns = [
            "carrier",
            "date",
            "output",
            "country",
            ]

        country = 'FR'  # Example country code
        ns_data = ns_data.loc[(ns_data['country']==country) & (ns_data['carrier']=='Wind')].reset_index(drop=True)
        ns_data['date'] = pd.to_datetime(ns_data['date'])
        # convert from gigawatt hours to kilowatt hours
        ns_data['output'] = pd.to_numeric(ns_data['output'])
        ns_data['output'] = ns_data['output'] * 1e6
        ns_data['year'] = ns_data['date'].dt.year.astype(int)
        ns_data['month'] = ns_data['date'].dt.month.astype(int)
        ns_data = ns_data.drop(columns=['date'])

        ns_data = ns_data.fillna(0).groupby(['year','month'])['output'].sum().reset_index()

        turb_info["ratio"] = turb_info['capacity'] / turb_info['capacity'].sum()

        ns_data = ns_data.merge(turb_info[['ID', 'ratio']], how="cross")
        ns_data["output"] = ns_data["output"] * ns_data["ratio"]
        ns_data = ns_data.dropna(subset=['ID', 'year', 'month'])
        ns_data = ns_data.loc[(ns_data["year"] >= year_star) & (ns_data["year"] <= year_end)].reset_index(drop=True)   
        obs_gen = ns_data.pivot(index=['ID','year'], columns='month', values='output').reset_index()
    
    elif country == "NL":
        # https://nationaalgeoregister.nl/geonetwork/srv/dut/catalog.search#/metadata/90f5eab6-9cea-4869-a031-2a228fb82fea
        nl_md = gpd.read_file("input/country-data/NL/nl_md.json").to_crs(epsg=4326)
        nl_md['lon'] = nl_md.geometry.x  
        nl_md['lat'] = nl_md.geometry.y
        nl_md = nl_md.drop(columns=['geometry','x','y','prov_naam','gem_naam','naam'])
        nl_md["ondergrond"] = nl_md["ondergrond"].replace({"land": "onshore", "zee": "offshore"})
        nl_md["land"] = nl_md["land"].replace({"België": "BE", "Duitsland": "DE", "Nederland": "NL"})
        nl_md.columns = [
            "ID",
            "diameter",
            "height",
            "capacity",
            "country",
            "manufacturer",
            "type",
            "lon",
            "lat"
        ]
        nl_md = nl_md.loc[nl_md['country']=='NL'].reset_index(drop=True).drop(columns=['country'])
        nl_md = nl_md[["ID","capacity","diameter","height","manufacturer","lon","lat","type"]]
        nl_md['manufacturer'] = nl_md['manufacturer'].str.split(' ').str[0].str.strip('123-.,').astype(str)
        turb_info = add_models(nl_md)
        
        # producing obs_gen
        if train == True:
            year_star = 2015  # start year of training period
            year_end = 2018  # end year of training period
            
        # this is for testing I use year_star and end for the sake of keeping code same
        else:
            year_star = year_test
            year_end = year_test
        
        ns_data = pd.read_csv("input/country-data/northsea_country_generation.csv")
        ns_data = ns_data.loc[
            :, [
            "Standard international energy product classification (SIEC)", 
            "TIME_PERIOD", 
            "OBS_VALUE",
            "geo",
            ]]
        ns_data.columns = [
            "carrier",
            "date",
            "output",
            "country",
            ]

        country = 'NL'  # Example country code
        ns_data = ns_data.loc[(ns_data['country']==country) & (ns_data['carrier']=='Wind')].reset_index(drop=True)
        ns_data['date'] = pd.to_datetime(ns_data['date'])
        # convert from gigawatt hours to kilowatt hours
        ns_data['output'] = pd.to_numeric(ns_data['output'])
        ns_data['output'] = ns_data['output'] * 1e6
        ns_data['year'] = ns_data['date'].dt.year.astype(int)
        ns_data['month'] = ns_data['date'].dt.month.astype(int)
        ns_data = ns_data.drop(columns=['date'])

        ns_data = ns_data.fillna(0).groupby(['year','month'])['output'].sum().reset_index()

        turb_info["ratio"] = turb_info['capacity'] / turb_info['capacity'].sum()

        ns_data = ns_data.merge(turb_info[['ID', 'ratio']], how="cross")
        ns_data["output"] = ns_data["output"] * ns_data["ratio"]
        ns_data = ns_data.dropna(subset=['ID', 'year', 'month'])
        ns_data = ns_data.loc[(ns_data["year"] >= year_star) & (ns_data["year"] <= year_end)].reset_index(drop=True)   
        obs_gen = ns_data.pivot(index=['ID','year'], columns='month', values='output').reset_index()
        
    else:
        raise ValueError("Country not recognised, please choose from: DK, DE, UK, FR")
    
    ####################################################################################
    ############# common processing for all countries to turn power into cf ############
    ####################################################################################
    # turning power into capacity factor
    obs_gen.columns = [f'obs_{i}' if i not in ['ID', 'year'] else f'{i}' for i in obs_gen.columns]
    obs_gen = obs_gen.merge(turb_info[['ID', 'capacity']], how='left', on=['ID'])
    obs_gen = obs_gen.dropna().reset_index(drop=True)

    def daysDuringMonth(yy, m):
        """
        Attach number of days in month to each year in yy for month m.
        """
        result = []    
        [result.append(monthrange(y, m)[1]) for y in yy]        
        return result

    # converting power to cf
    for i in range(1,13):
        obs_gen['obs_'+str(i)] = obs_gen['obs_'+str(i)]/(((daysDuringMonth(obs_gen.year, i))*obs_gen['capacity'])*24)
        
    obs_gen = obs_gen.drop(['capacity'], axis=1)

    return obs_gen, turb_info