import xarray as xr
import numpy as np
import pandas as pd
import difflib

import time
import utm
from calendar import monthrange
import datetime

from vwf.extras import (
    add_times,
    add_time_res
)

from vwf.simulation import simulate_wind

def prep_era5(train=False):
    """
    Reading and processing a saved ERA5 file with 100m wind speeds and fsr.

        Args:
            train (boolean): if for training or not
            
        Returns:
            ds (xarray.DataSet): dataset with wind variables on a grid
    """
    # load the corresponding raw ERA5 file
    if train == True:
        ds = xr.open_mfdataset('data/input/reanalysis/train/*.nc')
    else:
        ds = xr.open_mfdataset('data/input/reanalysis/test/*.nc')
    ds = ds.compute() # this allows it to not be dask chunks
    
    # converting wind speed components into wind speed
    ds["wnd100m"] = np.sqrt(ds["u100"] ** 2 + ds["v100"] ** 2).assign_attrs(
        units=ds["u100"].attrs["units"], long_name="100 metre wind speed"
    )
    
    ds = ds.drop_vars(["u100", "v100"])
    ds = ds.rename({"fsr": "roughness"})
    
    # turn hourly data into daily for speed of existing code
    ds = ds.resample(time='1D').mean()
    
    try:
        ds = ds.rename({"longitude": "lon", "latitude": "lat"})
    except:
        pass
        
    # keeping values at fixed float length
    ds = ds.assign_coords(
        lon=np.round(ds.lon.astype(float), 5), lat=np.round(ds.lat.astype(float), 5)
    )
    return ds
    
    
def add_models(df):
    """
    Assign model names to input turbines.
    
    Using our collection of models to assign power curves to observational data,
    matching is done via most similar power density, using the manufacturer if
    possible.

        Args:
            df (pandas.DataFrame): dataframe with input turbine metadata
            
        Returns:
            df (pandas.DataFrame): input df with added column called model
    """

    models = pd.read_csv('data/input/models.csv')
    models['model'] = models['model'].astype(pd.StringDtype())
    models['manufacturer'] = models['manufacturer'].str.lower()

    print("Total observed turbines/farms before conditions: ", len(df))
    
    # removing turbines that are unrealistic
    df = df.drop(df[df['height'] < 1].index).reset_index(drop=True)
    
    df['capacity'] = df['capacity'].astype(float)
    df['p_density'] = (df['capacity']*1000) / (np.pi * (df['diameter']/2)**2)
    df['capacity'] = df['capacity'].astype(int)
    df['ID'] = df['ID'].astype(str)
    
    # merging by manufacturer and power density if available
    if 'manufacturer' in df:
        df['manufacturer'] = df['manufacturer'].astype(pd.StringDtype())
        df['manufacturer'] = df['manufacturer'].str.lower()

        df = df.merge(models
            .assign(match=models['manufacturer'].apply(lambda x: difflib.get_close_matches(x, df['manufacturer'],cutoff=0.3,n=100)))
            .explode('match').drop_duplicates(),
            # .explode('manufacturer'),
            left_on=['manufacturer'], right_on=['match'],
            how="outer"
        )
        df = df.dropna(subset=['ID'])
        
        df = (
            df.assign(
                closest= np.abs(df['p_density_x'] - df['p_density_y'])
            )
            .sort_values("closest")
            .drop_duplicates(subset=["ID"], keep="first")
        )
        # allowing for better power density match if manufaturer model is not close enough
        df['model'].where(df['closest'] < 1, np.nan, inplace=True)
        df = df.drop(['diameter_y', 'p_density_y', 'offshore','manufacturer_y', 'capacity_y','match','closest','manufacturer_x'], axis=1)

    if 'type' in df.columns:
        df.columns = ['ID','capacity','diameter','height','lon','lat','type','p_density','model']
    else:
        df.columns = ['ID','capacity','diameter','height','lon','lat','p_density','model']
        df['type'] = 'onshore'
        
    df = df[['ID','type','capacity','diameter','height','lon','lat','model', 'p_density']]
    
    # matching on closest power density with a given tolerance
    df = df.sort_values('p_density').reset_index(drop=True)
    models = models.sort_values('p_density')
    df.loc[df['model'].isna(), 'model'] = pd.merge_asof(df, models, on='p_density', direction="nearest",tolerance=100)['model_y']

    df = df.dropna(subset=['model'])
    df = df.sort_values('ID').reset_index(drop=True)
    
    return df


def prep_country(country, train=False, *args):
    """
    Country specific preprocessing of observational data.
    
    Prepping the relevant countries observational data into a format that can
    be managed in `prep_obs()` each country will be unique here. Produce turb_info 
    then obs_gen.

        Args:
            country (str): country code e.g. Denmark "DK"
            
        Returns:
            obs_gen (pandas.DataFrame): obs_gen which is a time series of the power generated by a turbine
            turb_info (pandas.DataFrame): consists of the turbines metadata; latitude, longitude, capacity, height, rotor diameter and model
    """

    # for Denmark the metadata of existing turbines exist on anlaeg.xlsx,
    # sourced from Denmark Energy Agency. DK_md.csv is a tidied version.
    if country == "DK":
        # producing turb_info
        dk_md = pd.read_csv('data/input/country-data/DK/observations/DK_md.csv')
        # selecting relevent columns
        columns = ['Turbine identifier (GSRN)',
                'Manufacture','Capacity (kW)',
                'Rotor diameter (m)','Hub height (m)',
                'X (east) coordinate\nUTM 32 Euref89',
                'Y (north) coordinate\nUTM 32 Euref89', 
                'Type of location']
        dk_md = dk_md[columns]
        dk_md.columns = ['ID','manufacturer','capacity','diameter','height','x_east_32','y_north_32', 'type']

        # onshore or offshore
        dk_md['type'] = dk_md['type'].str.lower()
        dk_md.loc[dk_md['type'] == 'land', 'type'] = 'onshore'
        dk_md.loc[dk_md['type'] == 'hav', 'type'] = 'offshore'
        
        # convert coordinate system
        dk_md['x_east_32'] = pd.to_numeric(dk_md['x_east_32'],errors = 'coerce')
        dk_md['y_north_32'] = pd.to_numeric(dk_md['y_north_32'],errors = 'coerce')
        dk_md = dk_md.dropna(subset=['capacity', 'diameter', 'x_east_32','y_north_32']).reset_index(drop=True)
        
        def rule(row):
            lat, lon = utm.to_latlon(row["x_east_32"], row["y_north_32"], 32, 'W')
            return pd.Series({"lat": lat, "lon": lon})
        
        dk_md = dk_md.merge(dk_md.apply(rule, axis=1), left_index= True, right_index= True)
        
        dk_md = dk_md[['ID','manufacturer','capacity','diameter','height','lon','lat', 'type']]
        # getting first word of manufacturer name
        dk_md['manufacturer'] = dk_md['manufacturer'].str.split(' ').str[0]
        
        turb_info = add_models(dk_md)
    
        # producing obs_gen
        # load observation data and slice the observed power for chosen years
        if train == True:
            year_star = 2015 # start year of training period
            year_end = 2019 # end year of training period
            
        else: # this is for testing I use year_star and end for the sake of keeping code same
            year_test = args[0]
            year_star = year_test 
            year_end = year_test 
            
        appended_data = []
        for i in range(year_star, year_end+1):
            data = pd.read_excel('data/input/country-data/DK/observations/Denmark_'+str(i)+'.xlsx')
            data = data.iloc[3:,np.r_[0:1, 3:15]] # the slicing done here is file dependent please consider this when other files are used
            data.columns = ['ID','1','2','3','4','5','6','7','8','9','10','11','12']
            data['ID'] = data['ID'].astype(str)
            data = data.reset_index(drop=True)
            data['year'] = i
            appended_data.append(data[:-1])

        obs_gen = pd.concat(appended_data).reset_index(drop=True)
        obs_gen = obs_gen.fillna(0)

    # Germany data is sourced from Iain Staffell
    elif country == "DE":
        # producing turb_info
        de_geo = pd.read_csv('data/input/country-data/DE/observations/geolocate.germany.csv') # contains the postcode and lat lon
        de_md = pd.read_csv('data/input/country-data/DE/observations/DE_md.csv') # contains turbine metda data
        
        # selecting relevent columns
        de_md = de_md[['V1','Manufacturer','kW','Rotor..m.','Tower..m.']]
        de_md.columns = ['ID','manufacturer','capacity','diameter','height']
        de_md['postcode'] = de_md['ID'].astype(str).str[:5].astype(int)
    
        # geolocating by postcode
        de_md = pd.merge(de_md, de_geo[['postcode','lon','lat']], on='postcode', how='left')
        de_md = de_md.drop(["postcode"], axis=1)
        de_md = de_md.dropna(subset=['capacity', 'diameter', 'lon', 'lat']).reset_index(drop=True)
        
        turb_info = add_models(de_md)
        
        # producing obs_gen
        if train == True:
            year_star = 2011 # start year of training period
            year_end = 2014 # end year of training period
            
        else: # this is for testing I use year_star and end for the sake of keeping code same
            year_test = args[0]
            year_star = year_test 
            year_end = year_test 
            
        # load observation data and slice the observed power for chosen years
        de_data = pd.read_csv('data/input/country-data/DE/observations/DE_data.csv')
        de_data = de_data.loc[(de_data["Year"] >= year_star) & (de_data["Year"] <= year_end)].drop(['Downtime'], axis=1).reset_index(drop=True)   
        de_data.columns = ['ID','year','month','output']
        de_data = de_data.dropna(subset=['ID', 'year', 'month'])
        obs_gen = de_data.pivot(index=['ID','year'], columns='month', values='output').reset_index()
        obs_gen = obs_gen.fillna(0)
        
    return obs_gen, turb_info
            
def prep_obs(country, train=False, *args):
    """
    Preprocess the turbines/farms found in the observed data.
    
    Prepares the observational data (obs_cf and turb_info) for the desired country
    used in the model. Converts observed power generation into observed CF and ensures
    that all turbines in the data are acceptable.

        Args:
            country (str): country code e.g. Denmark "DK"
            
        Returns:
            obs_gen (pandas.DataFrame): obs_gen which is a time series of the power generated by a turbine
            turb_info (pandas.DataFrame): consists of the turbines metadata; latitude, longitude, capacity, height, rotor diameter and model
    """

    obs_gen, turb_info = prep_country(country, train, *args)

    # if train == True:
    obs_gen.columns = [f'obs_{i}' if i not in ['ID', 'year'] else f'{i}' for i in obs_gen.columns]
    # else:
    #     year_test = args[0]
    #     obs_gen.columns = [f'obs_{i}' if i not in ['ID'] else f'{i}' for i in obs_gen.columns]
    
    # converting obs_gen into obs_cf by turning power into capacity factor
    df = pd.merge(obs_gen, turb_info[['ID', 'capacity']],  how='left', on=['ID'])
    df = df.dropna().reset_index(drop=True)

    def daysDuringMonth(yy, m):
        result = []    
        [result.append(monthrange(y, m)[1]) for y in yy]        
        return result

    for i in range(1,13):
        df['obs_'+str(i)] = df['obs_'+str(i)]/(((daysDuringMonth(df.year, i))*df['capacity'])*24)
        
    # df = df.replace(0, np.nan) # converting back into NaN for ignoring in the next few steps
    df = df.drop(['capacity'], axis=1).reset_index(drop=True) # removing column that isn't needed
    
    # the conditions to determine if a turbine should be removed from the data
    # cf can't be > 1
    df['cf_max'] = df[df.columns[df.columns.str.startswith('obs')]].max(axis=1)
    df = df.drop(df[df['cf_max'] > 1].index)
    
    # removes all turbines that have a month of no CF
    # this needs to be adjusted to accomodate for countries that aren't denmark
    # ideally this should keep them but weigh how much it modifies the CF when averaging
    df['cf_min'] = df[df.columns[df.columns.str.startswith('obs')]].min(axis=1)
    if (train == True) & (country == 'DK'):
        df = df.drop(df[df['cf_min'] <= 0.01].index)
        
    # this is similar to above but removes if over a period the cf is less than 0.01
    df['cf_mean'] = df[df.columns[df.columns.str.startswith('obs')]].mean(axis=1)
    df = df.drop(df[df['cf_mean'] <= 0.01].index)
    obs_cf = df.drop(['cf_mean', 'cf_max' , 'cf_min'], axis=1).reset_index(drop=True)
    # obs_cf = df.drop(['cf_mean', 'cf_max'], axis=1).reset_index(drop=True)
    obs_cf = obs_cf.replace(0, np.nan) # converting back into NaN for ignoring in the next few steps
    
    # filtering the data to be ideal for training
    # the turbine should exist throughout the whole training period
    if train == True:
        year_star = obs_cf.year.min()
        year_end = obs_cf.year.max()
        obs_cf = obs_cf.loc[obs_cf['ID'].isin(turb_info['ID'])].reset_index(drop=True)
        obs_cf = obs_cf[obs_cf.groupby('ID').ID.transform('count') == ((year_end-year_star)+1)].reset_index(drop=True)
        obs_cf = obs_cf[['ID','year','obs_1','obs_2','obs_3','obs_4','obs_5','obs_6','obs_7','obs_8','obs_9','obs_10','obs_11','obs_12']]
        obs_cf.columns = ['ID','year','1','2','3','4','5','6','7','8','9','10','11','12']
        
        turb_info = turb_info.loc[turb_info['ID'].isin(obs_cf['ID'])].reset_index(drop=True)    
        obs_cf = obs_cf.melt(id_vars=["ID", "year"], 
                        var_name="month", 
                        value_name="obs")
                        
        obs_cf['month'] = obs_cf['month'].astype(int)
        obs_cf['year'] = obs_cf['year'].astype(int)
    
    # formatting for test scenarios
    else:
        # some random stuff to make it easier to plot for research
        obs_cf = obs_cf.drop('year', axis=1)
        year_test = args[0]
        dates = np.arange(str(year_test)+'-01', str(year_test+1)+'-01', dtype='datetime64[M]')
        cols = dates.tolist()
        obs_cf.columns = ['ID'] + cols
        obs_cf = obs_cf.loc[obs_cf['ID'].isin(turb_info['ID'])]
    
        turb_info = turb_info.loc[turb_info['ID'].isin(obs_cf['ID'])].reset_index(drop=True)
        obs_cf = obs_cf.set_index('ID').transpose().rename_axis('time').reset_index()
    
    print("Number of valid observed turbines/farms: ", len(turb_info))
    
    return obs_cf, turb_info


def merge_gen_cf(reanalysis, obs_cf, turb_info, powerCurveFile):
    
    sim_ws, sim_cf = simulate_wind(reanalysis, turb_info, powerCurveFile)
    sim_cf = sim_cf.groupby(pd.Grouper(key='time',freq='M')).mean().reset_index()
    sim_cf = sim_cf.melt(id_vars=["time"], 
                    var_name="ID", 
                    value_name="sim")
    sim_cf = add_times(sim_cf)
    sim_cf = add_time_res(sim_cf)

    sim_cf['ID'] = sim_cf['ID'].astype(str)
    obs_cf['ID'] = obs_cf['ID'].astype(str)
    
    gen_cf = pd.merge(sim_cf, obs_cf, on=['ID', 'month', 'year'], how='left')
    gen_cf = gen_cf.drop(['time'], axis=1).reset_index(drop=True)
    
    return gen_cf